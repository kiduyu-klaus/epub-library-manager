{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re  # Regular expression operations\n",
    "import time  # Time-related functions\n",
    "from urllib.request import urlopen  # URL opening function\n",
    "from bs4 import BeautifulSoup  # HTML parsing library\n",
    "import bs4  # Beautiful Soup library\n",
    "import requests  # HTTP library\n",
    "from pathlib import Path  # File system path operations\n",
    "import pandas as pd  # Data analysis and manipulation library\n",
    "import urllib.request as urllib  # URL request library\n",
    "from matplotlib.pyplot import title  # Matplotlib plotting library\n",
    "import requests  # HTTP library\n",
    "import urllib  # URL library\n",
    "import pandas as pd  # Data analysis and manipulation library\n",
    "from requests_html import HTML  # HTML parsing library\n",
    "from requests_html import HTMLSession  # HTML session library\n",
    "import os  # Operating system library\n",
    "import glob\n",
    "from os.path import exists  # File existence checking\n",
    "import json  # JSON manipulation library\n",
    "from datetime import datetime  # Date and time library\n",
    "import shutup  # Custom library (not recognized by standard Python)\n",
    "import csv  # CSV file operations library\n",
    "import sys  # System-specific parameters and functions\n",
    "from csv import writer  # CSV file writing library\n",
    "import random  # Random number generation library\n",
    "from requests_html import HTML  # HTML parsing library\n",
    "from requests_html import HTMLSession  # HTML session library\n",
    "import ebooklib  # E-book manipulation library\n",
    "from ebooklib import epub  # EPUB file manipulation library\n",
    "import epub_meta  # EPUB metadata extraction library\n",
    "import zipfile  # ZIP file manipulation library\n",
    "from lxml import etree  # XML processing library\n",
    "import click  # Command line interface library\n",
    "# Concurrent programming library\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading  # Thread-based programming library\n",
    "from langdetect import detect  # Language detection library\n",
    "import xml.etree.ElementTree as ET  # XML parsing library\n",
    "from urllib.request import urlopen, Request\n",
    "# Importing UserAgent from the fake_useragent library for generating fake user agents\n",
    "from fake_useragent import UserAgent\n",
    "# Importing the unescape function from the html module for unescaping HTML entities\n",
    "from html import unescape\n",
    "import subprocess   # Importing the subprocess module for managing subprocesses\n",
    "from tqdm import tqdm   # Importing tqdm for creating progress bars\n",
    "import socket   # Importing the socket module for low-level networking operations\n",
    "# Importing the get function from the requests library for making HTTP requests\n",
    "from requests import get\n",
    "# Importing the LibgenSearch class from the libgen_api library for interacting with the Library Genesis API\n",
    "from libgen_api import LibgenSearch\n",
    "from os import remove  # Importing the remove function from the os module for deleting files\n",
    "# Setting filters for book titles: language and extension\n",
    "title_filters = {\"Language\": \"English\", \"Extension\": \"epub\"}\n",
    "tf = LibgenSearch()  # Create an instance of the LibgenSearch class for interacting with the Library Genesis (Libgen) database\n",
    "noimage = \"https://t3.ftcdn.net/jpg/04/34/72/82/360_F_434728286_OWQQvAFoXZLdGHlObozsolNeuSxhpr84.jpg\"  # Default URL for the cover image if retrieval fails\n",
    "if sys.version_info >= (3,):   # Checking if the Python version is 3 or higher\n",
    "    # Importing urllib.request as urllib2 for Python 3\n",
    "    import urllib.request as urllib2\n",
    "    import urllib.parse as urlparse   # Importing urllib.parse as urlparse for Python 3\n",
    "else:\n",
    "    import urllib2   # Importing urllib2 for Python 2\n",
    "    import urlparse   # Importing urlparse for Python 2\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, HTTPError\n",
    "# Setting the maximum column width for pandas DataFrame to None, allowing unlimited width\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "import subprocess  # Import the subprocess module for executing external commands\n",
    "\n",
    "# Setting the URL for the subcategory image\n",
    "sub_cat_image = 'https://cdn-icons-png.flaticon.com/512/2702/2702069.png'\n",
    "# Setting the URL for the category image\n",
    "cat_image = 'https://cdn.iconscout.com/icon/free/png-512/free-ebook-1473378-1251457.png'\n",
    "\n",
    "status = 1   # Setting the status variable to 1\n",
    "show_on_home = 1   # Setting the show_on_home variable to 1\n",
    "\n",
    "# Setting the URL for the Goodreads website\n",
    "GOODREADS_URL = \"https://www.goodreads.com\"\n",
    "\n",
    "info = {}   # Initializing an empty dictionary called info\n",
    "\n",
    "EBOOK_HUNTER_URL=\"https://ebook-hunter.org\" # Setting the URL for the ebook-hunter website\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import quote_plus\n",
    "list_of_user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "            'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "            'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "            'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    "             \n",
    "            ]\n",
    "import os\n",
    "import fnmatch\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(category=urllib3.exceptions.InsecureRequestWarning)\n",
    "from googlesearch import search\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import shutil \n",
    "import mysql.connector\n",
    "\n",
    "import wikipedia\n",
    "from wikipedia.exceptions import PageError, DisambiguationError\n",
    "\n",
    "from libgen_api_enhanced import LibgenSearch\n",
    "s = LibgenSearch()\n",
    "\n",
    "from urllib.parse import urlsplit, urlunsplit ,urlparse, parse_qs\n",
    "\n",
    "import logging\n",
    "\n",
    "def setup_logging(log_filename):\n",
    "    \"\"\"\n",
    "    Set up the logging configuration.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): The name of the log file.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(filename=log_filename, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def log(log_filename, message, level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Log the provided message to a specified file with a given log level.\n",
    "\n",
    "    Args:\n",
    "        log_filename (str): The name of the log file.\n",
    "        message (str): The message to log.\n",
    "        level (int): The logging level (default: logging.INFO).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure logging to append to the specified file\n",
    "    logging.basicConfig(filename=log_filename, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    if level == logging.DEBUG:\n",
    "        logging.debug(message)\n",
    "    elif level == logging.INFO:\n",
    "        logging.info(message)\n",
    "    elif level == logging.WARNING:\n",
    "        logging.warning(message)\n",
    "    elif level == logging.ERROR:\n",
    "        logging.error(message)\n",
    "    elif level == logging.CRITICAL:\n",
    "        logging.critical(message)\n",
    "    \n",
    "OPENAI_API_KEY='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'\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "Gemini_key='AIzaSyCASKQRxoOKw3ZLgGIdEQHUc2d-E0EXUHQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change sid and id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configure Gemini AI with the API key from the environment variable\n",
    "genai.configure(api_key=Gemini_key)\n",
    "\n",
    "# Create the model configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 0.7,  # Lower temperature for deterministic responses\n",
    "    \"top_p\": 0.95,  # Use nucleus sampling\n",
    "    \"top_k\": 40,  # Consider top-k tokens\n",
    "    \"max_output_tokens\": 512,  # Limit response length\n",
    "    \"response_mime_type\": \"text/plain\",  # Expect text response\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash-exp\",  # Use the appropriate model name\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "def get_goodreads_title(book_title, author_name):\n",
    "    \"\"\"\n",
    "    Passes a book title and author to Gemini AI to get the Goodreads official book link.\n",
    "\n",
    "    Args:\n",
    "        book_title (str): The title of the book.\n",
    "        author_name (str): The name of the author.\n",
    "\n",
    "    Returns:\n",
    "        str: The Goodreads official book link or an error message.\n",
    "    \"\"\"\n",
    "    # Start a new chat session\n",
    "    chat_session = model.start_chat(history=[])\n",
    "\n",
    "    # Construct the input prompt\n",
    "    prompt = (\n",
    "        f\"Find the official Goodreads title for the book titled '{book_title}' \"\n",
    "        f\"written by the author '{author_name}'. Return only the goodreads title.\"\n",
    "    )\n",
    "\n",
    "    # Send the message to the Gemini model\n",
    "    response = chat_session.send_message(prompt)\n",
    "\n",
    "    # Extract the text response\n",
    "    response_text = response.text.strip()\n",
    "    if response_text.strip().lower() == book_title.strip().lower():\n",
    "        return None\n",
    "    return response_text\n",
    "\n",
    "    # Validate the output (basic validation for Goodreads link)\n",
    "    #if response_text.startswith(\"https://www.goodreads.com\"):\n",
    "        #return response_text\n",
    "    #else:\n",
    "        #return \"Could not retrieve a valid Goodreads link.\"\n",
    "\n",
    "def wiki_search(search_term, sentences=5):\n",
    "    \"\"\"\n",
    "    Performs a Wikipedia search and returns a summary for the given search term.\n",
    "    \n",
    "    Args:\n",
    "        search_term (str): The term to search for on Wikipedia.\n",
    "        sentences (int, optional): The number of sentences to include in the summary. Default is 2.\n",
    "        \n",
    "    Returns:\n",
    "        str: The summary of the Wikipedia page for the search term, or an error message if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform the Wikipedia search and get the summary\n",
    "        result = wikipedia.summary(search_term, sentences=sentences)\n",
    "        return result\n",
    "    \n",
    "    except PageError:\n",
    "        # Handle case where no page is found for the search term\n",
    "        return \"No Description available.\"\n",
    "    \n",
    "    except DisambiguationError as e:\n",
    "        # Handle disambiguation errors by suggesting possible options\n",
    "        return \"No Description available.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch-all for any other potential errors\n",
    "        return \"No Description available.\"\n",
    "\n",
    "def get_epub_info(fname):\n",
    "    \"\"\"\n",
    "    Extracts metadata from an EPUB file.\n",
    "\n",
    "    Args:\n",
    "        fname (str): The path to the EPUB file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted metadata, including 'title' and 'creator',\n",
    "              or None if an error occurs.\n",
    "    \"\"\"\n",
    "    ns = {\n",
    "        # Namespace for the container.xml file\n",
    "        'n': 'urn:oasis:names:tc:opendocument:xmlns:container',\n",
    "        'pkg': 'http://www.idpf.org/2007/opf',  # Namespace for the package metadata\n",
    "        'dc': 'http://purl.org/dc/elements/1.1/'  # Namespace for Dublin Core metadata\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Open the EPUB file\n",
    "        zip = zipfile.ZipFile(fname)\n",
    "\n",
    "        # Read the content of the container.xml file\n",
    "        txt = zip.read('META-INF/container.xml')\n",
    "        tree = etree.fromstring(txt)  # Parse the XML content\n",
    "        \n",
    "        # Extract the path of the contents metafile\n",
    "        cfname = tree.xpath('n:rootfiles/n:rootfile/@full-path', namespaces=ns)[0]\n",
    "\n",
    "        # Read the contents metafile\n",
    "        cf = zip.read(cfname)  # Read the contents metafile\n",
    "        tree = etree.fromstring(cf)  # Parse the XML content\n",
    "\n",
    "        # Extract the metadata block\n",
    "        p = tree.xpath('/pkg:package/pkg:metadata', namespaces=ns)[0]\n",
    "\n",
    "        # Repackage the data\n",
    "        res = {}  # Initialize a dictionary to store the metadata\n",
    "        for s in ['title', 'creator']:\n",
    "            # Extract the text content of each metadata element\n",
    "            res[s] = p.xpath(f'dc:{s}/text()', namespaces=ns)[0]\n",
    "\n",
    "        return res  # Return the metadata dictionary\n",
    "    \n",
    "    except (zipfile.BadZipFile, KeyError, etree.XMLSyntaxError, IndexError) as e:\n",
    "        # Handle specific exceptions (e.g., invalid EPUB format, missing metadata)\n",
    "        print(f\"Error reading EPUB file '{fname}': {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Handle other unexpected exceptions\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def appendbook_info_to_csv(epub_files_info, csv_file):\n",
    "    \"\"\"\n",
    "    Appends the given list of EPUB files information to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        epub_files_info (list of dict or dict): List of dictionaries containing book information or a single dictionary.\n",
    "        csv_file (str): The path to the CSV file to append the data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if a single dictionary is provided and convert it to a list of dictionaries\n",
    "    if isinstance(epub_files_info, dict):\n",
    "        epub_files_info = [epub_files_info]\n",
    "    \n",
    "    # Create a DataFrame from the provided data\n",
    "    df = pd.DataFrame(epub_files_info)\n",
    "\n",
    "    # Append the data to the CSV file\n",
    "    if os.path.isfile(csv_file):\n",
    "        # If the file exists, append without writing the header\n",
    "        df.to_csv(csv_file, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        # If the file does not exist, write with header\n",
    "        df.to_csv(csv_file, mode='w', index=False, header=True)\n",
    "\n",
    "def book_info(bookname, max_retries=3):\n",
    "    \"\"\"\n",
    "    Retrieves the Goodreads URL for a book based on its name and author by performing a search.\n",
    "    Retries the request in case of an HTTPError.\n",
    "\n",
    "    Args:\n",
    "        bookname (str): The name of the book to search for.\n",
    "        author_b (str): The author of the book. (This parameter is not used in the current implementation.)\n",
    "        max_retries (int): The maximum number of times to retry the request in case of an HTTPError.\n",
    "\n",
    "    Returns:\n",
    "        str: The URL of the book's Goodreads page, or a message indicating an error.\n",
    "    \n",
    "    Raises:\n",
    "        IndexError: If no matching book is found, causing list index out of range when accessing the first element of `matching`.\n",
    "    \"\"\"\n",
    "    base_url = 'https://www.goodreads.com/search?'\n",
    "    params = {'q': bookname}\n",
    "    print('searching for ',bookname)\n",
    "    search_url = base_url + urllib.parse.urlencode(params)\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Perform the search\n",
    "            search_page = urlopen(search_url)\n",
    "            search_html = search_page.read().decode(\"utf-8\")\n",
    "            search_soup = BeautifulSoup(search_html, \"html.parser\")\n",
    "\n",
    "            # Extract links\n",
    "            links_with_text = [a['href'] for a in search_soup.find_all('a', href=True) if a.text]\n",
    "\n",
    "            # Find the first book link\n",
    "            matching = [s for s in links_with_text if \"/book/show/\" in s]\n",
    "            \n",
    "            if not matching:\n",
    "                raise IndexError(\"No matching book found in search results.\")\n",
    "            \n",
    "            book_name = matching[0].split('?')[0]\n",
    "            complete_url = \"https://www.goodreads.com\" + book_name\n",
    "            print('=====> book found ')\n",
    "            return complete_url\n",
    "        \n",
    "        except HTTPError as e:\n",
    "            print(f\"HTTPError: {e.code} - {e.reason}. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "            attempt += 1\n",
    "        \n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError: {str(e)}\")\n",
    "            return \"Error: Book not found.\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {str(e)}\")\n",
    "            return \"Error: An unexpected error occurred.\"\n",
    "\n",
    "    # Return a message after exhausting retries\n",
    "    return \"Error: Unable to fetch book information after multiple attempts.\"\n",
    "\n",
    "def get_unique_aids_and_save(csv_filename, output_filename):\n",
    "    unique_aids = set()\n",
    "\n",
    "    # Read the CSV file and collect unique aids\n",
    "    with open(csv_filename, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            unique_aids.add(row['aid'])  # Add each aid to the set\n",
    "\n",
    "    # If the output file exists, read existing aids and add them to the set to avoid duplicates\n",
    "    if os.path.exists(output_filename):\n",
    "        with open(output_filename, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                unique_aids.add(row['aid'])  # Add existing aids to the set\n",
    "\n",
    "    # Write (or append) the unique aids to the output file\n",
    "    with open(output_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['aid'])  # Write the header\n",
    "        for aid in sorted(unique_aids):  # Sorting for better organization\n",
    "            writer.writerow([aid])\n",
    "\n",
    "def extract_search_query(url):\n",
    "    \"\"\"\n",
    "    Extracts the search query from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL containing the search query.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted search query, or None if no query is found.\n",
    "    \"\"\"\n",
    "    # Parse the URL and extract the query parameters\n",
    "    parsed_url = urlparse(url)\n",
    "    \n",
    "    # Parse the query string into a dictionary\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    \n",
    "    # Get the 'q' parameter, which is typically used for search queries\n",
    "    search_query = query_params.get('q')\n",
    "    \n",
    "    # Return the search query if it exists, otherwise return None\n",
    "    return search_query[0] if search_query else None\n",
    "\n",
    "def remove_extra_spaces(s):\n",
    "    \"\"\"\n",
    "    Removes extra spaces from the string and ensures only single spaces between words.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string with extra spaces.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned-up string with extra spaces removed.\n",
    "    \"\"\"\n",
    "    # Split the string into words, automatically removing extra spaces\n",
    "    words = s.split()\n",
    "    # Join the words with a single space\n",
    "    cleaned_string = ' '.join(words)\n",
    "    return cleaned_string\n",
    "\n",
    "def clean_url(url):\n",
    "    \"\"\"\n",
    "    Cleans the URL by removing everything after the '?'.\n",
    "\n",
    "    Args:\n",
    "        url (str): The input URL to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned URL with everything after '?' removed.\n",
    "    \"\"\"\n",
    "    # Parse the URL\n",
    "    parsed_url = urlparse(url)\n",
    "    # Construct the cleaned URL by using the scheme, netloc, path, and excluding the query and fragment\n",
    "    cleaned_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}\"\n",
    "    return cleaned_url\n",
    "\n",
    "def scrape_goodreads_books(url, author_name):\n",
    "    \"\"\"\n",
    "    Scrapes Goodreads books from the provided URL and compares each book's author(s) with the given author name.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the Goodreads search results page.\n",
    "        author_name (str): The author name to compare with the book authors.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of URLs of the books found on the page.\n",
    "    \"\"\"\n",
    "    source = urlopen(url)  # Open the URL and retrieve the HTML source\n",
    "    soup = BeautifulSoup(source, \"html.parser\")  # Parse the HTML with BeautifulSoup\n",
    "    setup_logging('log_filename.log')\n",
    "    # Find all book containers on the page\n",
    "    book_containers = soup.find_all('tr', itemtype='http://schema.org/Book')\n",
    "    \n",
    "    print(f'Results found: {len(book_containers)}')\n",
    "    #log(f'Results found: {len(book_containers)}')\n",
    "    log('log_filename.log', f'Results found: {len(book_containers)}', level=logging.INFO)\n",
    "    \n",
    "    # If only one result is found, return its URL immediately\n",
    "    if len(book_containers) == 1:\n",
    "        container = book_containers[0]\n",
    "        title_link = container.find('a', class_='bookTitle').get(\"href\")\n",
    "        complete_book_url = clean_url(GOODREADS_URL + title_link)\n",
    "        return [complete_book_url]\n",
    "    \n",
    "    # List to store all book URLs\n",
    "    complete_book_urls = []\n",
    "    search_query = extract_search_query(url)\n",
    "    \n",
    "    print(search_query)\n",
    "    log('log_filename.log',search_query, level=logging.WARNING)\n",
    "    \n",
    "    # Iterate over each book container\n",
    "    for container in book_containers:\n",
    "        # Extract book title link and title\n",
    "        title_link = container.find('a', class_='bookTitle').get(\"href\")\n",
    "        title = container.find('a', class_='bookTitle').text.strip()\n",
    "        \n",
    "        # Extract book author(s)\n",
    "        author_containers = container.find_all('a', class_='authorName')\n",
    "        authors = [author.text.strip() for author in author_containers]\n",
    "        \n",
    "        #print(authors)\n",
    "        log('log_filename.log',authors, level=logging.WARNING)\n",
    "        \n",
    "        # Check if the given author_name matches any of the extracted authors\n",
    "        if any(author_name.lower().strip() in remove_extra_spaces(author.lower().strip()) for author in authors):\n",
    "            # Extract the book rating\n",
    "            rating = container.find('span', class_='minirating').text.strip()\n",
    "                \n",
    "            # Extract the image URL\n",
    "            image_url_tag = container.find('img', itemprop='image')\n",
    "            image_url = image_url_tag['src'] if image_url_tag else 'No image available'\n",
    "                \n",
    "            # Construct the full book URL\n",
    "            complete_book_url = clean_url(GOODREADS_URL + title_link)\n",
    "            \n",
    "            # Add the complete book URL to the list\n",
    "            complete_book_urls.append(complete_book_url)\n",
    "            return complete_book_url\n",
    "        \n",
    "        complete_book_url = clean_url(GOODREADS_URL + title_link)\n",
    "        # Add the complete book URL to the list\n",
    "        complete_book_urls.append(complete_book_url)\n",
    "    if complete_book_urls:\n",
    "    # Return the list of all book URLs\n",
    "        return complete_book_urls[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_epub_and_cover(start_dir):\n",
    "    \"\"\"\n",
    "    Recursively searches for all EPUB files and JPG files in the specified directory and its subdirectories.\n",
    "    Checks if there is a JPG file in the same folder as the EPUB files.\n",
    "\n",
    "    Args:\n",
    "        start_dir (str): The root directory to start the search from.\n",
    "\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(start_dir):\n",
    "        # Loop through all files in the current directory\n",
    "        for file in files:\n",
    "            # Check if the file is an EPUB file\n",
    "            if file.endswith('.epub'):\n",
    "                new_file_name = file.replace(' ', '_')\n",
    "                book_url_local = new_file_name\n",
    "                new_creator_name=''\n",
    "                # Create the full path of the EPUB file\n",
    "                epub_path = os.path.join(root, file)\n",
    "                #print(\"EPUB Name:\", file)\n",
    "                \n",
    "                ebook_info= get_epub_info(epub_path)\n",
    "                ebook_title=ebook_info['title']\n",
    "                print('----> working on '+ebook_title)\n",
    "                ebook_name_clean = re.sub(r'\\(.*?\\)', '', ebook_title)\n",
    "                if ':' in ebook_name_clean:\n",
    "                    ebook_name_clean = ebook_name_clean.split(':')[0]\n",
    "                ebook_creator=ebook_info['creator']\n",
    "                new_creator_name=ebook_creator\n",
    "                if ',' in ebook_creator:\n",
    "                    creator_names= ebook_creator.split(',')\n",
    "                    first_name=creator_names[1].strip()\n",
    "                    second_name=creator_names[0].strip()\n",
    "                    new_creator_name=first_name+' '+second_name\n",
    "                    print(new_creator_name)\n",
    "                bookname=ebook_name_clean+' by '+new_creator_name\n",
    "                \n",
    "                # URL of the Goodreads page to scrape\n",
    "                base_url = 'https://www.goodreads.com/search?'\n",
    "                params = {'q': bookname}\n",
    "\n",
    "                search_url = base_url + urllib.parse.urlencode(params)\n",
    "                print(search_url)\n",
    "\n",
    "                # Call the function to scrape the book data\n",
    "                ebook_url = scrape_goodreads_books(search_url,new_creator_name.strip())\n",
    "                \n",
    "                #ebook_url=book_info(bookname, max_retries=3)\n",
    "                epub_files_info={\n",
    "                     'title': ebook_name_clean,\n",
    "                     'author': new_creator_name,\n",
    "                     'book_link': ebook_url,\n",
    "                     'book_url_local': book_url_local\n",
    "                    \n",
    "                 }\n",
    "                appendbook_info_to_csv(epub_files_info,'james_patterson.csv')\n",
    "                #time.sleep(5)\n",
    "                #print(\"EPUB Name: \"+ ebook_name_clean+\" EPUB Author: \"+new_creator_name)\n",
    "                print('============================> done')\n",
    "                \n",
    "def find_ebooks_files(root_dir):\n",
    "    \"\"\"\n",
    "    Recursively searches for all EPUB files in the specified directory and its subdirectories.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The root directory to start the search from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of full paths of EPUB files with spaces in file names replaced by underscores.\n",
    "    \"\"\"\n",
    "    epub_files = []\n",
    "\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in fnmatch.filter(files, '*.epub'):\n",
    "            # Replace spaces with underscores in the file name\n",
    "            new_file_name = file\n",
    "            # Construct the full path with the new file name\n",
    "            full_path = os.path.join(root, new_file_name)\n",
    "            epub_files.append(full_path)\n",
    "\n",
    "    return epub_files            \n",
    "        \n",
    "def is_book_in_csv(book_file_url, csv_file):\n",
    "    \"\"\"\n",
    "    Checks if a book with the given file URL is present in the specified CSV file.\n",
    "    \n",
    "    Args:\n",
    "        book_file_url (str): The file URL of the book to search for.\n",
    "        csv_file (str): The path to the CSV file.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the book is found in the CSV file, otherwise False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Check if the 'book_file_url' column contains the specified URL\n",
    "        if 'book_file_url' in df.columns:\n",
    "            if book_file_url in df['book_file_url'].values:\n",
    "                return True\n",
    "        else:\n",
    "            print(\"Error: The CSV file does not contain a 'book_file_url' column.\")\n",
    "            return False\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_file} does not exist.\")\n",
    "        return False\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: The file {csv_file} is empty.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return False\n",
    "    \n",
    "def do_google_search(query, num_results, driver):\n",
    "    # Perform a Google search with the specified query and number of results\n",
    "    driver.get(f\"https://www.google.com/search?q={query}&num={num_results}\")\n",
    "    WebDriverWait(driver, 15).until(ec.presence_of_element_located((By.CSS_SELECTOR, 'div.yuRUbf a')))\n",
    "    #time.sleep(5)  # Allow more time for all results to load\n",
    "\n",
    "    links = driver.find_elements(By.CSS_SELECTOR, 'div.yuRUbf a')\n",
    "    urls = [link.get_attribute(\"href\") for link in links if link.get_attribute(\"href\") and 'http' in link.get_attribute(\"href\")]\n",
    "    urls = [url for url in urls if 'translate.google.com' not in url]  # Exclude Google Translate links\n",
    "\n",
    "    unique_domains = set()\n",
    "    unique_urls = []\n",
    "    for url in urls:\n",
    "        domain = urlparse(url).netloc\n",
    "        if domain not in unique_domains:\n",
    "            unique_domains.add(domain)\n",
    "            unique_urls.append(url)\n",
    "\n",
    "    return urls\n",
    "\n",
    "def g_search(text):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--headless\")  # Ensure GUI is off\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options = chrome_options)\n",
    "    urls = do_google_search(text, 5, driver)\n",
    "    print(f'Found {len(urls)} URLs:')\n",
    "    for url in urls:\n",
    "        \n",
    "        if url.startswith('https://www.goodreads.com/book/show/') or url.startswith('https://www.goodreads.com/en/book/show'):\n",
    "            return url\n",
    "            \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    \n",
    "def get_sub_category_id(sub_category_name, csv_file='subcategories.csv'):\n",
    "    \"\"\"\n",
    "    Searches for a subcategory in the CSV file and returns its ID and associated category ID.\n",
    "    Exits the search once the subcategory is found.\n",
    "\n",
    "    Args:\n",
    "        sub_category_name (str): The subcategory name to search for.\n",
    "        csv_file (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'sid' and 'cat_id' if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Normalize the subcategory name\n",
    "    sub_category_name = sub_category_name.strip().lower()\n",
    "\n",
    "    try:\n",
    "        with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "\n",
    "            for row in reader:\n",
    "                # Normalize the subcategory name from the CSV file\n",
    "                csv_sub_category_name = row['sub_cat_name'].strip().lower()\n",
    "\n",
    "                if csv_sub_category_name == sub_category_name:\n",
    "                    return {\n",
    "                        'sid': int(row['sid']),  # Return the subcategory ID as an integer\n",
    "                        'cat_id': int(row['cat_id'])  # Return the category ID as an integer\n",
    "                    }\n",
    "                #print(f\"{sub_category_name} not found in {csv_file}.\")\n",
    "            # Return None if the subcategory is not found\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_file} does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_subcategories(sub_cat_id_string, csv_file='subcategories.csv'):\n",
    "    \"\"\"\n",
    "    Processes the sub_cat_id string from the book data, searches for each subcategory\n",
    "    in the CSV file, and returns the ID and category ID of the first found subcategory.\n",
    "\n",
    "    Args:\n",
    "        sub_cat_id_string (str): Comma-separated string of subcategory names.\n",
    "        csv_file (str): The path to the CSV file containing subcategories.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'sid', 'cat_id', and 'sub_cat_name' of the first found subcategory,\n",
    "              otherwise creates a new subcategory and returns its information.\n",
    "    \"\"\"\n",
    "    if sub_cat_id_string:\n",
    "        subcategories = [sub_cat.strip() for sub_cat in sub_cat_id_string.split(',')]\n",
    "        \n",
    "        # Skip specific genres if there are more than 2 subcategories including them\n",
    "        skip_genres = {'fiction', 'mystery', 'romance'}\n",
    "        filtered_subcategories = []\n",
    "        \n",
    "        for genre in skip_genres:\n",
    "            if genre in (sub.lower() for sub in subcategories):\n",
    "                count_including_genre = sum(1 for sub in subcategories if sub.lower() != genre)\n",
    "                if count_including_genre > 1:\n",
    "                    filtered_subcategories = [sub for sub in subcategories if sub.lower() != genre]\n",
    "                else:\n",
    "                    filtered_subcategories = subcategories\n",
    "            else:\n",
    "                filtered_subcategories = subcategories\n",
    "                \n",
    "        subcategories = filtered_subcategories\n",
    "\n",
    "        for subcategory in subcategories:\n",
    "            # Search for the subcategory ID\n",
    "            subcategory_info = get_sub_category_id(subcategory, csv_file)\n",
    "            if subcategory_info is not None:\n",
    "                # Return the first found subcategory with its ID and category ID\n",
    "                return {\n",
    "                    'sid': subcategory_info['sid'],\n",
    "                    'cat_id': subcategory_info['cat_id'],\n",
    "                    'sub_cat_name': subcategory\n",
    "                }\n",
    "\n",
    "        # If no subcategory is found, create a new one\n",
    "        new_subcategory_name = subcategories[0]  # Choose the first subcategory\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        if not df.empty:\n",
    "            last_sid = df['sid'].max()\n",
    "            new_sid = last_sid + 1\n",
    "        else:\n",
    "            new_sid = 1\n",
    "        \n",
    "        new_subcategory = {\n",
    "            'sid': new_sid,\n",
    "            'cat_id': 1,  # Assign a default category ID if needed\n",
    "            'sub_cat_name': new_subcategory_name,\n",
    "            'sub_cat_image': new_subcategory_name + '.jpg',\n",
    "            'status': 1\n",
    "        }\n",
    "        \n",
    "        # Append the new subcategory to the CSV file\n",
    "        new_df = pd.DataFrame([new_subcategory])\n",
    "        new_df.to_csv(csv_file, mode='a', index=False, header=False)\n",
    "        \n",
    "        return {\n",
    "            'sid': new_sid,\n",
    "            'cat_id': 1,\n",
    "            'sub_cat_name': new_subcategory_name\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        return {\n",
    "            'sid': 1,\n",
    "            'cat_id': 1,\n",
    "            'sub_cat_name': 'Default'\n",
    "        }\n",
    "    \n",
    "def get_genre_list(soup):\n",
    "    \"\"\"\n",
    "    Extracts genre information from a BeautifulSoup object representing an HTML page.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing genres with their corresponding 'sid' and 'found_cat_id'.\n",
    "    \"\"\"\n",
    "\n",
    "    genres = []\n",
    "    genres_container = soup.find(\"div\", {\"data-testid\": \"genresList\"})\n",
    "\n",
    "    if genres_container:\n",
    "        genre_links = genres_container.find(\"ul\").find(\"span\").find_all(\"a\")\n",
    "\n",
    "        for link in genre_links:\n",
    "            genre = link.find(\"span\").text\n",
    "            genres.append(genre)\n",
    "        categories_string = ', '.join(genres)\n",
    "        return categories_string\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_id(bookid):\n",
    "    \"\"\"\n",
    "    Extracts the ID from a book ID.\n",
    "\n",
    "    Args:\n",
    "        bookid (str): The book ID.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted ID.\n",
    "\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"([^.-]+)\")\n",
    "    bookdd = bookid.split('-')[0]\n",
    "    bookdd = bookdd.split('.')[0]\n",
    "    return bookdd\n",
    "\n",
    "def get_author_description(soup, id_number):\n",
    "    cell = soup.find(\"span\", {\"id\": \"freeTextContainerauthor\" + id_number})\n",
    "    if cell:\n",
    "        return cell.text.strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_author_id(soup):\n",
    "    \"\"\"\n",
    "    Retrieves the author ID from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        str: The author ID.\n",
    "\n",
    "    \"\"\"\n",
    "    # Find the anchor tag with the class \"ContributorLink\"\n",
    "    #author_url = soup.find(\"a\", {\"class\": \"ContributorLink\"})['href']\n",
    "    author_url22 = soup.find('a', class_='ContributorLink')\n",
    "    if author_url22:\n",
    "        author_url = author_url22['href']\n",
    "        # Split the author URL by \"/\" and take the last element as the author ID\n",
    "        author_id = author_url.split(\"/\")[-1]\n",
    "        # Split the author ID by \".\" and take the first element as the final author ID value\n",
    "        author_id = author_id.split(\".\")[0]\n",
    "        # Return the author ID\n",
    "        return author_id\n",
    "\n",
    "def get_book_description(soup):\n",
    "    \"\"\"\n",
    "    Retrieves the book description from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        str: The book description.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize an empty string for the book description\n",
    "    bdescription = ''\n",
    "    try:\n",
    "        # Check if the book description element exists\n",
    "        if soup.find(\"div\", {\"class\": \"DetailsLayoutRightParagraph__widthConstrained\"}).get_text():\n",
    "            # Retrieve the book description text\n",
    "            bdescription = soup.find(\n",
    "                \"div\", {\"class\": \"DetailsLayoutRightParagraph__widthConstrained\"}).get_text()\n",
    "            # Return the book description\n",
    "            return bdescription\n",
    "    except AttributeError:\n",
    "        # Return 'No Description available' if there is an AttributeError (element not found)\n",
    "        return 'No Description available'\n",
    "\n",
    "    # Return 'No Description available' if the book description is empty\n",
    "    return 'No Description available'\n",
    "\n",
    "def get_id_number(author_id):\n",
    "    \"\"\"\n",
    "    Extracts the ID number from an author ID.\n",
    "\n",
    "    Args:\n",
    "        author_id (str): The author ID.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted ID number.\n",
    "\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"([^.-]+)\")\n",
    "    aid = pattern.search(author_id).group()\n",
    "    author_split = aid.split(\".\")\n",
    "    author_url1 = author_split[0]\n",
    "    return author_url1\n",
    "\n",
    "def get_author_image(soup, author_name):\n",
    "    \"\"\"\n",
    "    Extracts the image URL of an author from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "        author_name (str): The name of the author used to locate the image.\n",
    "\n",
    "    Returns:\n",
    "        str: The URL of the author image if found, or a placeholder image URL otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "    cell = soup.find(\"img\", {\"alt\": author_name, \"itemprop\": \"image\"})\n",
    "    if cell:\n",
    "        return cell.attrs.get(\"src\")\n",
    "    return 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/No-Image-Placeholder.svg/1665px-No-Image-Placeholder.svg.png'\n",
    "\n",
    "def get_author_info(soup):\n",
    "    \"\"\"Get all information from an author (genres, influences, website etc.).\n",
    "    Args:\n",
    "        soup (bs4.element.Tag): author page connection.\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    container = soup.find('div', attrs={'class': 'rightContainer'})\n",
    "    author_info = {}\n",
    "    data_div = container.find('br', attrs={'class': 'clear'})\n",
    "    while data_div:\n",
    "        if data_div.name:\n",
    "            data_class = data_div.get('class')[0]\n",
    "            # Information section is finished\n",
    "            if data_class == 'aboutAuthorInfo':\n",
    "                break\n",
    "            # Key elements\n",
    "            elif data_class == 'dataTitle':\n",
    "                key = data_div.text.strip()\n",
    "                author_info[key] = []\n",
    "            # Born section\n",
    "            if data_div.text == 'Born':\n",
    "                data_div = data_div.next_sibling\n",
    "                author_info[key].append(data_div.strip())\n",
    "            # Influences section\n",
    "            elif data_div.text == 'Influences':\n",
    "                data_div = data_div.next_sibling.next_sibling\n",
    "                data_items = data_div.findAll('span')[-1].findAll('a')\n",
    "                for data_a in data_items:\n",
    "                    author_info[key].append(data_a.text.strip())\n",
    "            # Member since section\n",
    "            elif data_div.text == 'Member Since':\n",
    "                data_div = data_div.next_sibling.next_sibling\n",
    "                author_info[key].append(data_div.text.strip())\n",
    "            # Genre, website and other sections\n",
    "            else:\n",
    "                data_items = data_div.findAll('a')\n",
    "                for data_a in data_items:\n",
    "                    author_info[key].append(data_a.text.strip())\n",
    "        data_div = data_div.next_sibling\n",
    "    return author_info\n",
    "\n",
    "def author_youtube_search(Author_name):\n",
    "    \"\"\"\n",
    "    Search for the author's YouTube channel using the specified author's name.\n",
    "    \n",
    "    Args:\n",
    "        Author_name (str): The author's name.\n",
    "        \n",
    "    Returns:\n",
    "        str: The YouTube channel link if found, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append the author's name and additional keywords to the search query\n",
    "        search_txt = Author_name + ' channel youtube'\n",
    "        \n",
    "        # Perform the search with a limit of 10 results\n",
    "        search_results = search(search_txt, num_results=10)\n",
    "        \n",
    "        # Look for a YouTube link in the search results\n",
    "        for result in search_results:\n",
    "            if 'https://www.youtube.com' in result:\n",
    "                return result\n",
    "        \n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    # Return an empty string if no YouTube link is found or an error occurred\n",
    "    return ''\n",
    "\n",
    "def author_instagram_search(Author_name):\n",
    "    \"\"\"\n",
    "    Search for the author's official Instagram account using the specified author's name.\n",
    "    \n",
    "    Args:\n",
    "        Author_name (str): The author's name.\n",
    "        \n",
    "    Returns:\n",
    "        str: The Instagram account link if found, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append the author's name and additional keywords to the search query\n",
    "        search_txt = Author_name + ' instagram official'\n",
    "        \n",
    "        # Perform the search with a limit of 10 results\n",
    "        search_results = search(search_txt, num_results=10)\n",
    "        \n",
    "        # Look for an Instagram link in the search results\n",
    "        for result in search_results:\n",
    "            if 'https://www.instagram.com' in result:\n",
    "                return result\n",
    "    \n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    # Return an empty string if no Instagram link is found or an error occurred\n",
    "    return ''\n",
    "\n",
    "def author_facebook_search(Author_name):\n",
    "    \"\"\"\n",
    "    Search for the author's official Facebook page using the specified author's name.\n",
    "    \n",
    "    Args:\n",
    "        Author_name (str): The author's name.\n",
    "        \n",
    "    Returns:\n",
    "        str: The Facebook page link if found, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append the author's name and additional keywords to the search query\n",
    "        search_txt = Author_name + ' facebook official'\n",
    "        \n",
    "        # Perform the search with a limit of 10 results\n",
    "        search_results = search(search_txt, num_results=10)\n",
    "        \n",
    "        # Look for a Facebook link in the search results\n",
    "        for result in search_results:\n",
    "            if 'https://www.facebook.com' in result:\n",
    "                return result\n",
    "\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError occurred while searching Facebook: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while searching Facebook: {e}\")\n",
    "    \n",
    "    # Return an empty string if no Facebook link is found or an error occurred\n",
    "    return ''\n",
    "\n",
    "def author_website_search(author_name):\n",
    "    \"\"\"\n",
    "    Search for the author's official website using the specified author's name.\n",
    "    \n",
    "    Args:\n",
    "        author_name (str): The author's name.\n",
    "        \n",
    "    Returns:\n",
    "        str: The website link if found, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append the author's name and additional keywords to the search query\n",
    "        search_txt = author_name + ' official website'\n",
    "        \n",
    "        # Perform the search with a limit of 10 results\n",
    "        search_results = search(search_txt, num_results=10)\n",
    "        \n",
    "        # Return the first valid result\n",
    "        for result in search_results:\n",
    "            return result\n",
    "\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError occurred while searching for the website: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while searching for the website: {e}\")\n",
    "    \n",
    "    # Return an empty string if no website link is found or an error occurred\n",
    "    return ''\n",
    "\n",
    "def scrape_author(author_id):\n",
    "    \"\"\"\n",
    "    Scrapes the author information from the Goodreads website.\n",
    "\n",
    "    Args:\n",
    "        author_id (str): The author ID.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the scraped author information.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://www.goodreads.com/author/show/\" + author_id\n",
    "\n",
    "    time.sleep(3)  # Pause execution for 3 seconds\n",
    "\n",
    "    source = urlopen(url)  # Open the URL and retrieve the HTML source\n",
    "    soup = BeautifulSoup(source, \"html.parser\")  # Create a BeautifulSoup object for parsing the HTML\n",
    "\n",
    "    author_name = soup.find(\"span\", {\"itemprop\": \"name\"}).text.strip()  # Extract the author name from the HTML\n",
    "    id_number = get_id_number(author_id)  # Call a helper function to get the ID number\n",
    "\n",
    "    author_info = get_author_info(soup)  # Call a helper function to get additional author information\n",
    "    if author_info:\n",
    "        if 'Born' in author_info:\n",
    "            author_city_name = author_info[\"Born\"][0]  # Extract the author's city name if available\n",
    "        else:\n",
    "            author_city_name = 'No City Name Found'\n",
    "\n",
    "        if 'Website' in author_info:\n",
    "            author_website = author_info[\"Website\"]  # Extract the author's website if available\n",
    "        else:\n",
    "            author_website = 'none'\n",
    "    else:\n",
    "        author_city_name = 'No City Name Found'\n",
    "        author_website = 'none'\n",
    "\n",
    "    #info[\"author_name\"] = author_name  # Add the author name to the 'info' dictionary\n",
    "    \n",
    "    author_des= get_author_description(soup, id_number)\n",
    "     \n",
    "\n",
    "    return {\n",
    "        \"author_id\": id_number,\n",
    "        \"author_name\": author_name,\n",
    "        \"author_city_name\": author_city_name,\n",
    "        \"author_description\": author_des,  # Call a helper function to get the author description\n",
    "        \"author_image\": get_author_image(soup, author_name),  # Call a helper function to get the author image\n",
    "        \"author_youtube\": author_youtube_search(author_name),  # Call a helper function to search for the author on YouTube\n",
    "        \"author_instagram\": author_instagram_search(author_name),  # Call a helper function to search for the author on Instagram\n",
    "        \"author_facebook\": author_facebook_search(author_name),  # Call a helper function to search for the author on Facebook\n",
    "        \"author_website\": author_website_search(author_name),  # Call a helper function to search for the author's website\n",
    "        \"status\": '1',\n",
    "    }\n",
    "\n",
    "def get_book_cover(soup):\n",
    "    \"\"\"\n",
    "    Retrieves the book cover image URL from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        str: The URL of the book cover image.\n",
    "\n",
    "    \"\"\"\n",
    "    if soup.find('img', {'class': 'ResponsiveImage'}):\n",
    "        cover = soup.find('img', {'class': 'ResponsiveImage'})\n",
    "        return cover.attrs.get('src')\n",
    "    #if soup.find(id=\"coverImage\"):\n",
    "        #cover = soup.find(id=\"coverImage\")\n",
    "       # print(cover)\n",
    "       # return cover.get('src')  # img.get\n",
    "    return 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/No-Image-Placeholder.svg/1665px-No-Image-Placeholder.svg.png'\n",
    "\n",
    "def scrape_book(book_url, book_url_local,random_header):\n",
    "    \"\"\"\n",
    "    Scrapes book information from a Goodreads book URL.\n",
    "\n",
    "    Args:\n",
    "        book_url (str): The Goodreads book URL.\n",
    "        book_url_local (str): The local URL of the book file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the scraped book information.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    url = book_url\n",
    "    source = requests.get(url,headers = random_header)\n",
    "    #print(source.status_code)\n",
    "    time.sleep(2)\n",
    "    soup = bs4.BeautifulSoup(source.content, 'html.parser')\n",
    "    book_title_element = soup.find('h1', class_='Text Text__title1', attrs={'data-testid': 'bookTitle'})\n",
    "    book_title =  ' '.join(book_title_element.text.split())\n",
    "    book_id_beta = book_url.replace('https://www.goodreads.com/book/show/', '')\n",
    "    book_id_beta1 = book_id_beta.replace(\n",
    "                'https://www.goodreads.com/en/book/show/', '')\n",
    "    g_list=get_genre_list(soup)\n",
    "    #print(g_list)\n",
    "    category_ids = process_subcategories(g_list)\n",
    "    sid = category_ids.get('sid')\n",
    "    cat_id = category_ids.get('cat_id')\n",
    "    #sid = 41\n",
    "    #cat_id = 3\n",
    "    \n",
    "    return {\n",
    "                'id': get_id(book_id_beta1),\n",
    "                'cat_id': cat_id,\n",
    "                'sub_cat_id': sid,\n",
    "                'aid': get_author_id(soup),\n",
    "                'featured': '1',\n",
    "                'book_title': book_title,\n",
    "                'book_description': get_book_description(soup),\n",
    "                'book_cover_img': get_book_cover(soup),\n",
    "                'book_bg_img': get_book_cover(soup),\n",
    "                'book_file_type': 'epub',\n",
    "                'book_file_url': book_url_local,\n",
    "                'total_rate': soup.find('span', {'data-testid': 'ratingsCount'}).text.strip().replace('\\xa0ratings',''),\n",
    "                'rate_avg': soup.find('div', {'class': 'RatingStatistics__rating'}).text.strip(),\n",
    "                'book_views': soup.find('span', {'data-testid': 'reviewsCount'}).text.strip().replace('\\xa0reviews',''),\n",
    "                'status': '1'\n",
    "\n",
    "            }\n",
    "\n",
    "def append_to_book_csv(book_data, csv_file):\n",
    "    \"\"\"\n",
    "    Appends the book data to the specified CSV file.\n",
    "\n",
    "    Args:\n",
    "        book_data (dict): A dictionary containing book details.\n",
    "        csv_file (str): The path to the CSV file where the data should be appended.\n",
    "    \"\"\"\n",
    "    fieldnames = ['id', 'cat_id', 'sub_cat_id', 'aid', 'featured', 'book_title', \n",
    "                  'book_description', 'book_cover_img', 'book_bg_img', 'book_file_type',\n",
    "                  'book_file_url', 'total_rate', 'rate_avg', 'book_views', 'status']\n",
    "\n",
    "    # Check if the file exists to write the header only if it's a new file\n",
    "    try:\n",
    "        with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header only if file is empty or does not exist\n",
    "            if file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "\n",
    "            # Write the book data\n",
    "            writer.writerow(book_data)\n",
    "            print(f\"Appended data for book '{book_data['book_title']}' to {csv_file}.\")\n",
    "    \n",
    "    except IOError as e:\n",
    "        print(f\"An I/O error occurred: {e}\")\n",
    "        \n",
    "def append_author_to_csv(file_name, author_data):\n",
    "    \"\"\"\n",
    "    Appends the scraped author data to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the CSV file.\n",
    "        author_data (dict): A dictionary containing the scraped author information.\n",
    "    \"\"\"\n",
    "    fieldnames = [\n",
    "        'author_id', 'author_name', 'author_city_name', 'author_description',\n",
    "        'author_image', 'author_youtube', 'author_instagram', 'author_facebook',\n",
    "        'author_website', 'status'\n",
    "    ]\n",
    "\n",
    "    # Check if the file exists and write header if not\n",
    "    try:\n",
    "        with open(file_name, mode='r', newline='', encoding='utf-8') as file:\n",
    "            pass\n",
    "    except FileNotFoundError:\n",
    "        with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "    # Append author data to the CSV file\n",
    "    with open(file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writerow(author_data)\n",
    "\n",
    "def add_spaces_before_capitals(filename):\n",
    "    \"\"\"\n",
    "    Adds spaces before each capital letter in the filename, excluding the file extension.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename of the ebook, e.g., 'BeyondEverAfter.epub'.\n",
    "\n",
    "    Returns:\n",
    "        str: The filename with spaces added before each capital letter, excluding the file extension.\n",
    "    \"\"\"\n",
    "    # Remove the file extension\n",
    "    base_name = filename.rsplit('.', 1)[0]\n",
    "\n",
    "    # Add spaces before each capital letter\n",
    "    spaced_name = re.sub(r'(?<!^)(?<!\\s)([A-Z])', r' \\1', base_name)\n",
    "\n",
    "    return spaced_name \n",
    "\n",
    "def remove_text_between_parentheses(text):\n",
    "    # Remove text between parentheses and also any text after the first opening parenthesis\n",
    "    cleaned_text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    # Remove any text after the first opening parenthesis\n",
    "    cleaned_text = re.sub(r'\\(.*', '', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def insert_book_data(data):\n",
    "    try:\n",
    "        # Connect to the MySQL database\n",
    "        conn = mysql.connector.connect(\n",
    "            host='localhost',\n",
    "            user='root',\n",
    "            password='',\n",
    "            database='php_web_services'\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # SQL query to insert data into tbl_books\n",
    "        sql_query = \"\"\"\n",
    "        INSERT INTO tbl_books \n",
    "        (id, cat_id, sub_cat_id, aid, featured, book_title, book_description, book_cover_img, \n",
    "         book_bg_img, book_file_type, book_file_url, total_rate, rate_avg, book_views, status)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        # Data to be inserted (from the returned dictionary)\n",
    "        values = (\n",
    "            data['id'], \n",
    "            data['cat_id'], \n",
    "            data['sub_cat_id'], \n",
    "            data['aid'], \n",
    "            int(data['featured']), \n",
    "            data['book_title'], \n",
    "            data['book_description'], \n",
    "            data['book_cover_img'], \n",
    "            data['book_bg_img'], \n",
    "            data['book_file_type'], \n",
    "            data['book_file_url'], \n",
    "            int(data['total_rate'].replace(',', '')), \n",
    "            float(data['rate_avg']), \n",
    "            int(data['book_views'].replace(',', '')), \n",
    "            int(data['status'])\n",
    "        )\n",
    "\n",
    "        # Execute the query and commit the transaction\n",
    "        cursor.execute(sql_query, values)\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Book '{data['book_title']}' inserted successfully.\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "def check_and_append_aid(aid, csv_file='aid1.csv'):\n",
    "    \"\"\"\n",
    "    Checks if an author ID exists in the CSV file, and if not, appends it on a new line.\n",
    "    If the file is empty, adds a header before appending the ID.\n",
    "\n",
    "    Args:\n",
    "        aid (str): The author ID to search for.\n",
    "        csv_file (str): The path to the CSV file (default is 'aid1.csv').\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the ID was appended, False if it already existed.\n",
    "    \"\"\"\n",
    "    aid_exists = False\n",
    "    \n",
    "    # Check if the file exists and is empty\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    file_empty = os.path.getsize(csv_file) == 0 if file_exists else True\n",
    "\n",
    "    # Read the CSV and check if the ID exists\n",
    "    if not file_empty:\n",
    "        try:\n",
    "            with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    if row['aid'] == str(aid):\n",
    "                        aid_exists = True\n",
    "                        break\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {csv_file} not found, creating a new one.\")\n",
    "\n",
    "    # If the ID doesn't exist, append it to the file\n",
    "    if not aid_exists:\n",
    "        with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if file_empty:  # Add header if the file is empty or newly created\n",
    "                writer.writerow(['aid'])\n",
    "            writer.writerow([aid])  # Append the ID on a new line\n",
    "        print(f\"Appended ID {aid} to {csv_file}.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"ID {aid} already exists in {csv_file}.\")\n",
    "        return False\n",
    "\n",
    "def move_and_rename_epub(source_path, destination_directory, new_name):\n",
    "    \"\"\"\n",
    "    Move an EPUB file from the source path to the destination directory and rename it.\n",
    "\n",
    "    Args:\n",
    "        source_path (str): The path to the source EPUB file.\n",
    "        destination_directory (str): The path to the destination directory.\n",
    "        new_name (str): The new name for the EPUB file (including .epub extension).\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the newly moved and renamed EPUB file.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(source_path):\n",
    "        print(f\"Source file '{source_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.isdir(destination_directory):\n",
    "        print(f\"Destination directory '{destination_directory}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    new_file_path = os.path.join(destination_directory, new_name)\n",
    "\n",
    "    try:\n",
    "        shutil.move(source_path, new_file_path)\n",
    "        print(f\"File moved and renamed to '{new_file_path}'.\")\n",
    "        return new_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving or renaming file: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_book_search(book_name, book_url_local, author_name, current_csv, list_of_user_agents, file):\n",
    "    \"\"\"\n",
    "    Process a book search to find and scrape book information from Goodreads.\n",
    "\n",
    "    Parameters:\n",
    "    - book_name (str): The name of the book to search for.\n",
    "    - book_url_local (str): The local URL or filename of the book.\n",
    "    - author_name (str): The author of the book.\n",
    "    - current_csv (str): The name of the CSV file to append book information.\n",
    "    - list_of_user_agents (list): A list of user agent strings for HTTP requests.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    text = book_name + ' goodreads book show'\n",
    "    result_book_url = g_search(text)\n",
    "    dest = r'E:\\The ToDo Network\\Ebook project files\\2024 lets do this again\\genres\\uploads'\n",
    "\n",
    "    if result_book_url:\n",
    "        handle_result_book_url(result_book_url, book_name, book_url_local, author_name, current_csv, list_of_user_agents, file, dest)\n",
    "    else:\n",
    "        handle_no_result_book_url(book_name, book_url_local, author_name, current_csv, list_of_user_agents, file, dest)\n",
    "\n",
    "def handle_result_book_url(result_book_url, book_name, book_url_local, author_name, current_csv, list_of_user_agents, file, dest):\n",
    "    \"\"\"\n",
    "    Handle the case where a valid book URL is already found and process it.\n",
    "\n",
    "    Parameters:\n",
    "    - result_book_url (str): The URL of the book to scrape.\n",
    "    - book_name (str): The name of the book.\n",
    "    - book_url_local (str): The local URL or filename of the book.\n",
    "    - author_name (str): The name of the book's author.\n",
    "    - current_csv (str): The CSV file to store book data.\n",
    "    - list_of_user_agents (list): A list of user agent strings for HTTP requests.\n",
    "    - file (str): The path to the eBook file being processed.\n",
    "    - dest (str): The destination directory for moving and renaming the eBook file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Inform the user that the book link has been found\n",
    "    print('Link for ' + str(book_url_local) + ' found at ' + str(result_book_url))\n",
    "\n",
    "    # Select a random user agent for the HTTP request to mimic browser behavior\n",
    "    user_agent = random.choice(list_of_user_agents)\n",
    "    random_header = {'User-Agent': user_agent}\n",
    "\n",
    "    try:\n",
    "        # Scrape book details from the provided URL\n",
    "        book_info = scrape_book(result_book_url, book_url_local, random_header)\n",
    "\n",
    "        # Check if the book's ID is already in the database or the current CSV\n",
    "        if not search_book_id(book_info['id']) and not check_book_id(book_info['id'], current_csv):\n",
    "            # Append the author ID to a dedicated CSV if it is not already present\n",
    "            authorid = book_info[\"aid\"]\n",
    "            check_and_append_aid(authorid, csv_file='aid1.csv')\n",
    "            \n",
    "            # Log that the book scraping was successful\n",
    "            print('Book scraped ----> ' + book_info['id'])\n",
    "\n",
    "            # Append book details to the main CSV file\n",
    "            append_to_book_csv(book_info, current_csv)\n",
    "\n",
    "            # Move the eBook file to the specified destination and rename it\n",
    "            move_and_rename_epub(file, dest, book_url_local)\n",
    "            print('=========================> Scraping ' + book_name + ' done')\n",
    "        else:\n",
    "            # If the book is already added, skip processing and delete the file\n",
    "            print(f\"{book_info['book_title']} already added.....skipping..\")\n",
    "            os.remove(file)\n",
    "            print('book deleted')\n",
    "\n",
    "    except ConnectionError as e:\n",
    "        # Handle connection errors, particularly issues with HTTPS connections\n",
    "        if 'HTTPSConnectionPool' in str(e):\n",
    "            print(f\"ConnectionError: HTTPSConnectionPool occurred for {book_name}. Skipping this book.\")\n",
    "            # Save the book details to a failed books log for later review\n",
    "            save_failed_book(book_name, book_url_local, author_name, file, result_book_url)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle other exceptions during the scraping process\n",
    "        print(f\"An error occurred while scraping the book {book_name}: {e}\")\n",
    "        # Save the book details to a failed books log for later review\n",
    "        save_failed_book(book_name, book_url_local, author_name, file, result_book_url)\n",
    "\n",
    "def split_title_and_author(book_info):\n",
    "    \"\"\"\n",
    "    Splits a book string into the title and author.\n",
    "\n",
    "    Args:\n",
    "        book_info (str): A string containing the book title and author in the format 'Title by Author'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the title and author as strings.\n",
    "    \"\"\"\n",
    "    # Split the string by the delimiter \" by \"\n",
    "    parts = book_info.rsplit(\" by \", 1)  # Use rsplit to ensure the split occurs at the last \" by \"\n",
    "\n",
    "    if len(parts) == 2:\n",
    "        title, author = parts\n",
    "        return title.strip(), author.strip()\n",
    "    else:\n",
    "        raise ValueError(\"Input string format is incorrect. Expected format: 'Title by Author'.\")\n",
    "      \n",
    "def handle_no_result_book_url(book_name, book_url_local, author_name, current_csv, list_of_user_agents, file, dest):\n",
    "    \"\"\"\n",
    "    Handle the case where no result book URL is found and attempt to search on Goodreads.\n",
    "\n",
    "    Parameters:\n",
    "    - book_name (str): The name of the book to process.\n",
    "    - book_url_local (str): The local URL or filename of the book.\n",
    "    - author_name (str): The author of the book.\n",
    "    - current_csv (str): The name of the CSV file to append book information.\n",
    "    - list_of_user_agents (list): A list of user agent strings for HTTP requests.\n",
    "    - file (str): The path to the eBook file being processed.\n",
    "    - dest (str): The destination directory for moving and renaming the eBook file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Inform the user that the function is attempting a Goodreads search\n",
    "    print('Trying Goodreads search again')\n",
    "    \n",
    "    # Construct the Goodreads search URL with the book name as a query parameter\n",
    "    base_url = 'https://www.goodreads.com/search?'\n",
    "    params = {'q': book_name}\n",
    "    search_url = base_url + urllib.parse.urlencode(params)\n",
    "    print(search_url)\n",
    "\n",
    "    # Scrape Goodreads for the book's URL based on the search results and author name\n",
    "    ebook_url = scrape_goodreads_books(search_url, author_name)\n",
    "    if ebook_url:\n",
    "        # If multiple URLs are returned, take the first one\n",
    "        if isinstance(ebook_url, list):\n",
    "            ebook_url = ebook_url[0]\n",
    "        print('Link for ' + str(book_url_local) + ' found at ' + str(ebook_url))\n",
    "\n",
    "        # Randomly select a user-agent header for the HTTP request\n",
    "        user_agent = random.choice(list_of_user_agents)\n",
    "        random_header = {'User-Agent': user_agent}\n",
    "\n",
    "        try:\n",
    "            # Scrape detailed book information from the retrieved Goodreads URL\n",
    "            book_info = scrape_book(ebook_url, book_url_local, random_header)\n",
    "\n",
    "            # Check if the book's ID is already present in the database or CSV\n",
    "            if not search_book_id(book_info['id']) and not check_book_id(book_info['id'], current_csv):\n",
    "                # Handle the author ID: append it to a CSV if it is not already present\n",
    "                authorid = book_info[\"aid\"]\n",
    "                check_and_append_aid(authorid, csv_file='aid1.csv')\n",
    "                \n",
    "                # Log successful scraping of the book\n",
    "                print('Book scraped ----> ' + book_info['id'])\n",
    "                \n",
    "                # Append the scraped book info to the main CSV\n",
    "                append_to_book_csv(book_info, current_csv)\n",
    "                \n",
    "                # Move and rename the eBook file to the specified destination\n",
    "                move_and_rename_epub(file, dest, book_url_local)\n",
    "                print('=========================> Scraping ' + book_name + ' done')\n",
    "            else:\n",
    "                # If the book is already in the database, skip and delete the file\n",
    "                print(f\"{book_info['book_title']} already added.....skipping.\")\n",
    "                os.remove(file)\n",
    "                print('book deleted')\n",
    "        except ConnectionError as e:\n",
    "            # Handle connection errors, specifically for HTTPS issues\n",
    "            if 'HTTPSConnectionPool' in str(e):\n",
    "                # Uncomment the following line to save failed book data\n",
    "                # save_failed_book(book_name, book_url_local, author_name, file, result_book_url)\n",
    "                print(f\"{book_url_local} skipped.\")\n",
    "        except Exception as e:\n",
    "            # Handle generic errors during the scraping process\n",
    "            print(f\"An error occurred while scraping the book {book_name}: {e}\")\n",
    "            # Uncomment the following line to save failed book data\n",
    "            # save_failed_book(book_name, book_url_local, author_name, file, result_book_url)\n",
    "    else:\n",
    "        # If no URL is found on Goodreads, skip processing for this book\n",
    "        print(f\"=====>>>>> {book_name} not found .... trying advanced search.\")\n",
    "        title1, author1 = split_title_and_author(book_name)\n",
    "        new_book_name=get_goodreads_title(title1, author1)\n",
    "        if new_book_name:\n",
    "            new_book_and_author=new_book_name+' by '+author1\n",
    "            process_book_search(new_book_and_author, book_url_local, author_name, current_csv, list_of_user_agents, file)\n",
    "                \n",
    "        else:\n",
    "           print(f\"{book_url_local} skipped.\")     \n",
    "        \n",
    "def copy_and_rename_epub(source_path, destination_directory, new_name):\n",
    "    \"\"\"\n",
    "    Copy an EPUB file from the source path to the destination directory and rename it.\n",
    "\n",
    "    Args:\n",
    "        source_path (str): The path to the source EPUB file.\n",
    "        destination_directory (str): The path to the destination directory.\n",
    "        new_name (str): The new name for the EPUB file (including .epub extension).\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the newly copied and renamed EPUB file.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(source_path):\n",
    "        print(f\"Source file '{source_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.isdir(destination_directory):\n",
    "        print(f\"Destination directory '{destination_directory}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    new_file_path = os.path.join(destination_directory, new_name)\n",
    "\n",
    "    try:\n",
    "        shutil.copy(source_path, new_file_path)\n",
    "        print(f\"File copied and renamed to '{new_file_path}'.\")\n",
    "        return new_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying or renaming file: {e}\")\n",
    "        return None\n",
    "\n",
    "def search_book_id(book_id,csv_file = 'tbl_books.csv'):\n",
    "    \"\"\"\n",
    "    Search for a specific book ID in the 'tbl_books.csv' file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing book data.\n",
    "        book_id (int or str): The book ID to search for in the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the book ID is found, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file cannot be found or opened.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            \n",
    "            # Loop through each row in the CSV\n",
    "            for row in reader:\n",
    "                # Check if the current row's 'id' matches the given book_id\n",
    "                if row['id'] == str(book_id):\n",
    "                    return True  # Return True if the book_id is found\n",
    "        return False  # Return False if not found\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_file} was not found.\")\n",
    "        return False            \n",
    "\n",
    "def check_book_id(book_id,csv_file):\n",
    "    \"\"\"\n",
    "    Search for a specific book ID in the 'tbl_books.csv' file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing book data.\n",
    "        book_id (int or str): The book ID to search for in the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the book ID is found, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file cannot be found or opened.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            \n",
    "            # Loop through each row in the CSV\n",
    "            for row in reader:\n",
    "                # Check if the current row's 'id' matches the given book_id\n",
    "                if row['id'] == str(book_id):\n",
    "                    return True  # Return True if the book_id is found\n",
    "        return False  # Return False if not found\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_file} was not found.\")\n",
    "        return False            \n",
    "\n",
    "def is_author_id_in_csv(aid, csv_file= 'tbl_author.csv'):\n",
    "    try:\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                if row[0] == str(aid):  # Check if the first column matches the author ID\n",
    "                    return True\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {csv_file} not found.\")\n",
    "        return False\n",
    "\n",
    "def split_epub_filename(epub_filename):\n",
    "    \"\"\"\n",
    "    Splits an EPUB filename into title and author.\n",
    "    \n",
    "    Args:\n",
    "        epub_filename (str): The EPUB filename.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the 'title' and 'author' as separate strings.\n",
    "    \"\"\"\n",
    "    # Remove the file extension (.epub)\n",
    "    filename = os.path.splitext(epub_filename)[0]\n",
    "    \n",
    "    # Find the last hyphen to split title and author\n",
    "    last_hyphen_index = filename.rfind(' - ')\n",
    "    \n",
    "    if last_hyphen_index != -1:\n",
    "        title = filename[:last_hyphen_index].strip()\n",
    "        author = filename[last_hyphen_index + 3:].strip()  # Skip past the hyphen and spaces\n",
    "    else:\n",
    "        # If no hyphen is found, treat the whole filename as the title\n",
    "        title = filename.strip()\n",
    "        author = 'Unknown'\n",
    "\n",
    "    # Remove the author's name from the title if it appears in it\n",
    "    title_without_author = title.replace(author, '').strip()\n",
    "    \n",
    "    pattern = r'\\b\\d+\\s*(?=\\w)'\n",
    "\n",
    "    # Remove any leading character or number before the title\n",
    "    title_cleaned = re.sub(r'^[^A-Za-z0-9]*(.*)', r'\\1', title_without_author).strip()\n",
    "    title_cleaned1 = re.sub(pattern, '',  title_cleaned).strip()\n",
    "    title_cleaned2 = re.sub(r'^[\\d.]+\\s*', '', title_cleaned1).strip()\n",
    "\n",
    "    return {\n",
    "        'title': clean_ebook_filename(title_cleaned2),\n",
    "        'author': author\n",
    "    }\n",
    "\n",
    "def clean_ebook_filename(filename):\n",
    "    \"\"\"\n",
    "    Removes specified substrings from an ebook filename.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The original ebook filename.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned ebook filename.\n",
    "    \"\"\"\n",
    "    # Remove everything between ( and ) including the parentheses\n",
    "    cleaned_filename = re.sub(r'\\s*\\(.*?\\)\\s*', '', filename)\n",
    "    \n",
    "    \n",
    "    cleaned_filename = re.sub(r'^[\\d.]+\\s*', '', filename)\n",
    "\n",
    "    # Remove everything after just ( including it\n",
    "    cleaned_filename = re.sub(r'\\s*\\(.*', '', cleaned_filename)\n",
    "\n",
    "    # Remove everything after the first - or _ (including them)\n",
    "    cleaned_filename = re.sub(r'\\s*[-_].*', '', cleaned_filename)\n",
    "\n",
    "    # Strip any extra spaces\n",
    "    return cleaned_filename.strip()\n",
    "\n",
    "def search_folder_by_name(folder_name, root_dir=r'E:\\Novels Library\\DCIM\\Calibre Library'):\n",
    "    \"\"\"\n",
    "    Searches for a folder by name within the specified root directory and returns its path.\n",
    "    This function only searches in the root directory and not in its subdirectories.\n",
    "\n",
    "    Args:\n",
    "        folder_name (str): The name of the folder to search for.\n",
    "        root_dir (str): The root directory to start the search from.\n",
    "\n",
    "    Returns:\n",
    "        str: The full path to the folder if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # List all items in the root directory\n",
    "    try:\n",
    "        root_items = os.listdir(root_dir)\n",
    "    except PermissionError as e:\n",
    "        print(f\"Permission error accessing directory: {root_dir}. {e}\")\n",
    "        return None\n",
    "    except FileNotFoundError as e:\n",
    "        #print(f\"Directory not found: {root_dir}. {e}\")\n",
    "        return None\n",
    "\n",
    "    # Check if the folder_name is in the list of directories\n",
    "    if folder_name in root_items:\n",
    "        folder_path = os.path.join(root_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            return folder_path\n",
    "    \n",
    "    return None  # Return None if the folder is not found\n",
    "\n",
    "def count_epub_files_in_folder_recursive(folder_path):\n",
    "    \"\"\"\n",
    "    Counts the number of EPUB files in the specified folder and its subdirectories.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder where the EPUB files are located.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of EPUB files in the folder and its subdirectories.\n",
    "    \"\"\"\n",
    "    epub_count = 0\n",
    "\n",
    "    try:\n",
    "        # Traverse the folder and subdirectories using os.walk\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            # Filter for files that end with .epub\n",
    "            epub_files = [file for file in files if file.lower().endswith('.epub')]\n",
    "            epub_count += len(epub_files)\n",
    "    except PermissionError as e:\n",
    "        print(f\"Permission error accessing directory: {folder_path}. {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Directory not found: {folder_path}. {e}\")\n",
    "\n",
    "    return epub_count\n",
    "\n",
    "def check_author_name(author_name, csv_file='tbl_author.csv'):\n",
    "    \"\"\"\n",
    "    Search for a specific author name in the 'tbl_author.csv' file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing author data.\n",
    "        author_name (str): The author name to search for in the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the author name is found, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file cannot be found or opened.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            \n",
    "            # Loop through each row in the CSV\n",
    "            for row in reader:\n",
    "                # Check if the current row's 'author_name' matches the given author_name\n",
    "                if row['author_name'].strip().lower() == author_name.strip().lower():\n",
    "                    return True  # Return True if the author_name is found\n",
    "        return False  # Return False if not found\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_file} was not found.\")\n",
    "        return False \n",
    "    \n",
    "def count_ebooks_by_author(root_folder):\n",
    "    \"\"\"\n",
    "    Scans the root folder and its subfolders organized by author\n",
    "    to count the number of .epub files for each author.\n",
    "\n",
    "    Args:\n",
    "        root_folder (str): The path to the root folder.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are author names and values are counts of .epub files.\n",
    "    \"\"\"\n",
    "    author_ebook_count = {}\n",
    "\n",
    "    # Iterate over folders in the root directory\n",
    "    for author_folder in os.listdir(root_folder):\n",
    "        author_path = os.path.join(root_folder, author_folder)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(author_path):\n",
    "            epub_count = 0\n",
    "\n",
    "            # Scan all subfolders and files in the author's directory\n",
    "            for root, _, files in os.walk(author_path):\n",
    "                # Count .epub files\n",
    "                epub_count += sum(1 for file in files if file.endswith('.epub'))\n",
    "\n",
    "            # Store the count with the author's name\n",
    "            author_ebook_count[author_folder] = epub_count\n",
    "\n",
    "    return author_ebook_count\n",
    "\n",
    "def is_author_id_in_local_csv(aid, csv_file):\n",
    "    \"\"\"\n",
    "    Checks if a given author ID exists in the specified CSV file.\n",
    "\n",
    "    Args:\n",
    "        aid (int or str): The author ID to search for.\n",
    "        csv_file (str): The path to the CSV file where the author ID is stored.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the author ID is found in the CSV file, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the CSV file in read mode, specifying UTF-8 encoding for compatibility\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)  # Create a CSV reader object to read rows from the file\n",
    "            for row in reader:\n",
    "                # Check if the first column matches the provided author ID\n",
    "                if row[0] == str(aid):  # Convert 'aid' to string for comparison\n",
    "                    return True  # Return True if a matching author ID is found\n",
    "        return False  # Return False if the author ID is not found after reading all rows\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the CSV file does not exist\n",
    "        print(f\"File {csv_file} not found.\")\n",
    "        return False\n",
    "\n",
    "def check_filename(file_name, csv_file='tbl_books.csv'):\n",
    "    try:\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if row['book_file_url'] == file_name:\n",
    "                    return True\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {csv_file} was not found.\")\n",
    "        return False\n",
    "    except KeyError:\n",
    "        print(f\"The column 'book_file_url' does not exist in {csv_file}.\")\n",
    "        return False\n",
    "\n",
    "def count_ebooks_by_author(root_folder):\n",
    "    \"\"\"\n",
    "    Scans the root folder and its subfolders organized by author\n",
    "    to count the number of .epub files for each author.\n",
    "\n",
    "    Args:\n",
    "        root_folder (str): The path to the root folder.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples where each tuple contains the author name,\n",
    "              count of .epub files, and the author's folder path.\n",
    "    \"\"\"\n",
    "    author_ebook_data = []\n",
    "\n",
    "    # Iterate over folders in the root directory\n",
    "    for author_folder in os.listdir(root_folder):\n",
    "        author_path = os.path.join(root_folder, author_folder)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(author_path):\n",
    "            epub_count = 0\n",
    "\n",
    "            # Scan all subfolders and files in the author's directory\n",
    "            for root, _, files in os.walk(author_path):\n",
    "                # Count .epub files\n",
    "                epub_count += sum(1 for file in files if file.endswith('.epub'))\n",
    "\n",
    "            # Store the data as a tuple (author name, count, folder path)\n",
    "            author_ebook_data.append((author_folder, epub_count, author_path))\n",
    "\n",
    "    return author_ebook_data\n",
    "\n",
    "def save_to_csv(data, csv_filename):\n",
    "    \"\"\"\n",
    "    Saves the list of author ebook data to a CSV file,\n",
    "    sorted by the number of books in descending order.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of tuples containing author names, ebook counts, and folder paths.\n",
    "        csv_filename (str): The name of the CSV file to save the data.\n",
    "    \"\"\"\n",
    "    # Sort the data by ebook count in descending order\n",
    "    sorted_data = sorted(data, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Author', 'Ebook Count', 'Folder Path'])  # Header\n",
    "        for author, count, path in sorted_data:\n",
    "            writer.writerow([author, count, path])\n",
    "\n",
    "def get_authors_with_book_count(csv_filename, min_count, max_count):\n",
    "    \"\"\"\n",
    "    Reads the CSV file and returns the folder paths of authors\n",
    "    whose ebook counts are between the specified range.\n",
    "\n",
    "    Args:\n",
    "        csv_filename (str): The path to the CSV file.\n",
    "        min_count (int): The minimum number of books (inclusive).\n",
    "        max_count (int): The maximum number of books (inclusive).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of folder paths for authors within the specified book count range.\n",
    "    \"\"\"\n",
    "    folder_paths = []\n",
    "\n",
    "    with open(csv_filename, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        # Iterate through each row in the CSV file\n",
    "        for row in reader:\n",
    "            # Parse the book count as an integer\n",
    "            book_count = int(row['Ebook Count'])\n",
    "            \n",
    "            # Check if the book count is within the specified range\n",
    "            if min_count <= book_count <= max_count:\n",
    "                folder_paths.append(row['Folder Path'])\n",
    "\n",
    "    return folder_paths\n",
    "\n",
    "def append_csv_data(source_csv):\n",
    "    \"\"\"\n",
    "    Appends all data from source_csv to the predefined target CSV without adding headers.\n",
    "    \n",
    "    :param source_csv: Name of the source CSV file in the specified directory.\n",
    "    \"\"\"\n",
    "    base_path = r\"E:\\The ToDo Network\\Ebook project files\\2024 lets do this again\\genres\\books\"\n",
    "    source_csv_path = f\"{base_path}\\\\{source_csv}\"  # Full path to the source CSV\n",
    "    target_csv = f\"{base_path}\\\\combined_csv.csv\"   # Full path to the target CSV\n",
    "\n",
    "    try:\n",
    "        with open(source_csv_path, mode='r', newline='', encoding='utf-8') as src_file:\n",
    "            reader = csv.reader(src_file)\n",
    "            # Skip headers in the source file\n",
    "            headers = next(reader, None)\n",
    "\n",
    "            with open(target_csv, mode='a', newline='', encoding='utf-8') as tgt_file:\n",
    "                writer = csv.writer(tgt_file)\n",
    "                writer.writerows(reader)  # Append rows from source file\n",
    "\n",
    "        print(f\"Data from {source_csv_path} successfully appended to {target_csv}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def count_books_by_aid(aid_to_check):\n",
    "    \"\"\"\n",
    "    Counts the number of books for a given author ID in the 'tbl_books.csv' dataset.\n",
    "\n",
    "    Args:\n",
    "        aid_to_check (str): The author ID to count books for.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of books for the given author ID.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        with open('tbl_books.csv', \"r\", encoding=\"utf-8\") as books_file:\n",
    "            books_csv = csv.DictReader(books_file)\n",
    "            for row in books_csv:\n",
    "                if row['aid'] == aid_to_check:\n",
    "                    count += 1\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred reading tbl_books.csv: {e}\")\n",
    "\n",
    "    return count\n",
    "\n",
    "def save_failed_book(book_name, book_url_local, author_name, file, result_book_url):\n",
    "    failed_books_file = 'failedbooks.csv'\n",
    "    if not os.path.exists(failed_books_file):\n",
    "        with open(failed_books_file, mode='w', newline='', encoding='utf-8') as failed_file:\n",
    "            writer = csv.writer(failed_file)\n",
    "            writer.writerow(['book_name', 'book_url_local', 'author_name', 'file', 'result_book_url'])  # Write header\n",
    "\n",
    "    # Save failed book details to failedbooks.csv\n",
    "    with open(failed_books_file, mode='a', newline='', encoding='utf-8') as failed_file:\n",
    "        writer = csv.writer(failed_file)\n",
    "        writer.writerow([book_name, book_url_local, author_name, file, result_book_url])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Specify the source and destination folders\n",
    "target_folder = r\"E:\\The ToDo Network\\Ebook project files\\2024 lets do this again\\genres\\26_t0_35\"\n",
    "destination_folder = r\"E:\\The ToDo Network\\Ebook project files\\2024 lets do this again\\old books\"\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Loop through the target folder and its subfolders\n",
    "for root, _, files in os.walk(target_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".epub\"):  # Check if the file has a .epub extension\n",
    "            source_path = os.path.join(root, file)\n",
    "            destination_path = os.path.join(destination_folder, file)\n",
    "            \n",
    "            try:\n",
    "                shutil.move(source_path, destination_path)\n",
    "                print(f\"Moved: {source_path} -> {destination_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error moving {source_path}: {e}\")\n",
    "\n",
    "print(\"Finished moving .epub files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> working on The Scarab's Command\n",
      "Found 5 URLs:\n",
      "Trying Goodreads search again\n",
      "https://www.goodreads.com/search?q=The+Scarab%27s+Command+by+Craig+Halloran\n",
      "Results found: 0\n",
      "The Scarab's Command by Craig Halloran\n",
      "=====>>>>> The Scarab's Command by Craig Halloran not found .... trying advanced search.\n",
      "The_Scarab's_Command_-_Craig_Halloran.epub skipped.\n",
      "Processed 1 books so far.\n"
     ]
    }
   ],
   "source": [
    "# Define the starting path\n",
    "starts = r'H:\\My Drive\\genre_codes\\8_to_9'\n",
    "current_folder = os.path.basename(starts)\n",
    "\n",
    "# Create the 'books' directory if it doesn't exist\n",
    "books_folder = os.path.join(os.getcwd(), 'books')\n",
    "os.makedirs(books_folder, exist_ok=True)\n",
    "\n",
    "# Set the path for the current CSV file in the 'books' directory\n",
    "current_csv = os.path.join(books_folder, current_folder.replace(' ', '_') + '.csv')\n",
    "\n",
    "# Create the CSV file if it doesn't exist\n",
    "if not os.path.exists(current_csv):\n",
    "    with open(current_csv, mode='w', newline='') as csv_file:\n",
    "        pass\n",
    "\n",
    "# Initialize the counter for processed books\n",
    "processed_count = 0\n",
    "\n",
    "# Loop through all files in the starting directory\n",
    "for root, dirs, files in os.walk(starts):\n",
    "    for file in files:\n",
    "        if file.endswith('.epub') or file.endswith('.ePub'):\n",
    "            new_file_name11 = file.replace('-OceanofPDF.com-', '')\n",
    "            new_file_name22 = new_file_name11.replace('_OceanofPDF.com_', '')\n",
    "            new_file_name = new_file_name22.replace(' ', '_')\n",
    "            book_url_local = new_file_name\n",
    "            new_creator_name = ''\n",
    "            \n",
    "            if not is_book_in_csv(book_url_local, current_csv):\n",
    "                epub_path = os.path.join(root, file)\n",
    "                author_title = split_epub_filename(new_file_name22)\n",
    "                \n",
    "                if author_title:\n",
    "                    ebook_title = author_title['title']\n",
    "                    print(f'----> working on {ebook_title}')\n",
    "                    ebook_creator = author_title['author']\n",
    "                    new_creator_name = ebook_creator\n",
    "                    ebook_name_clean = re.sub(r'\\(.*?\\)', '', ebook_title)\n",
    "                    bookname = ebook_name_clean + ' by ' + new_creator_name\n",
    "                    \n",
    "                    # Process the book search\n",
    "                    process_book_search(bookname, book_url_local, new_creator_name, current_csv, list_of_user_agents, epub_path)\n",
    "                    \n",
    "                    # Increment the processed books count\n",
    "                    processed_count += 1\n",
    "                    print(f\"Processed {processed_count} books so far.\")\n",
    "                else:\n",
    "                    print(f\"{book_url_local} info corrupted.. skipping.\")\n",
    "                    pass\n",
    "            else:\n",
    "                print(f\"{book_url_local} is already in the CSV file.\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage\n",
    "book_title = \"ef368c4ab81ffc49\"\n",
    "book_author = \"Jeaniene Frost\"\n",
    "\n",
    "goodreads_link = get_goodreads_title(book_title, book_author)\n",
    "print(f\"Goodreads link: {goodreads_link}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = r\"E:\\Novels Library\\DCIM\\Calibre Library\"\n",
    "ebook_data = count_ebooks_by_author(root_directory)\n",
    "\n",
    "# Save the sorted results to a CSV file\n",
    "csv_file = \"authorcount.csv\"\n",
    "save_to_csv(ebook_data, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file1 = 'book_paths.csv'\n",
    "csv_file = csv.reader(open(csv_file1, \"r\", encoding='utf-8'), delimiter=\",\")\n",
    "for row in csv_file:\n",
    "    if row:\n",
    "        path = row[2].strip()\n",
    "        starts=path\n",
    "        current_folder = os.path.basename(starts)\n",
    "        # Create the 'books' directory if it doesn't exist\n",
    "        books_folder = os.path.join(os.getcwd(), 'books')\n",
    "        os.makedirs(books_folder, exist_ok=True)\n",
    "\n",
    "        # Set the path for the current CSV file in the 'books' directory\n",
    "        current_csv = os.path.join(books_folder, current_folder.replace(' ', '_') + '.csv')\n",
    "\n",
    "        if not os.path.exists(current_csv):\n",
    "            # Create an empty CSV file\n",
    "            with open(current_csv, mode='w', newline='') as csv_file:\n",
    "                pass\n",
    "            \n",
    "        for root, dirs, files in os.walk(starts):\n",
    "            # Loop through all files in the current directory\n",
    "            for file in files:\n",
    "                # Check if the file is an EPUB file\n",
    "                if file.endswith('.epub') or file.endswith('.ePub'):\n",
    "                    new_file_name11 = file.replace('-OceanofPDF.com-', '')\n",
    "                    new_file_name22 = new_file_name11.replace('_OceanofPDF.com_', '')\n",
    "                    new_file_name = new_file_name22.replace(' ', '_')\n",
    "                    # print(new_file_name)\n",
    "                    book_url_local = new_file_name\n",
    "                    new_creator_name = ''\n",
    "                    if not is_book_in_csv(book_url_local, current_csv):\n",
    "                        # Create the full path of the EPUB file\n",
    "                        epub_path = os.path.join(root, file)\n",
    "                        # print(\"EPUB Name:\", file)\n",
    "                        author_title = split_epub_filename(new_file_name22)\n",
    "                        if author_title:\n",
    "                            # print(author_title['title'])\n",
    "                            ebook_title = author_title['title']\n",
    "                            print('----> working on ' + ebook_title)\n",
    "                            ebook_creator = author_title['author']\n",
    "                            new_creator_name = ebook_creator\n",
    "                            ebook_name_clean = re.sub(r'\\(.*?\\)', '', ebook_title)\n",
    "                            bookname = ebook_name_clean + ' by ' + new_creator_name\n",
    "                            process_book_search(bookname, book_url_local, new_creator_name, current_csv, list_of_user_agents, epub_path)\n",
    "                            # print(bookname)\n",
    "                        else:\n",
    "                            print(f\"{book_url_local} info corrupted.. skipping.\")\n",
    "                            pass\n",
    "                    else:\n",
    "                        print(f\"{book_url_local} is already in the CSV file.\")\n",
    "                        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_epub_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts=r'E:\\The ToDo Network\\Ebook project files\\2024 lets do this again\\genres\\26_t0_35'\n",
    "current_folder = os.path.basename(starts)\n",
    "# Create the 'books' directory if it doesn't exist\n",
    "books_folder = os.path.join(os.getcwd(), 'books')\n",
    "os.makedirs(books_folder, exist_ok=True)\n",
    "\n",
    "# Set the path for the current CSV file in the 'books' directory\n",
    "current_csv = os.path.join(books_folder, current_folder.replace(' ', '_') + '.csv')\n",
    "\n",
    "if not os.path.exists(current_csv):\n",
    "    # Create an empty CSV file\n",
    "    with open(current_csv, mode='w', newline='') as csv_file:\n",
    "        pass\n",
    "\n",
    "for root, dirs, files in os.walk(starts):\n",
    "    # Loop through all files in the current directory\n",
    "    for file in files:\n",
    "        # Check if the file is an EPUB file\n",
    "        if file.endswith('.epub') or file.endswith('.ePub'):\n",
    "                new_file_name11 = file.replace('-OceanofPDF.com-', '')\n",
    "                new_file_name22 = new_file_name11.replace('_OceanofPDF.com_', '')\n",
    "                new_file_name = new_file_name22.replace(' ', '_')\n",
    "                #print(new_file_name)\n",
    "                book_url_local = new_file_name\n",
    "                new_creator_name=''\n",
    "                if not is_book_in_csv(book_url_local,current_csv):\n",
    "                    # Create the full path of the EPUB file\n",
    "                    epub_path = os.path.join(root, file)\n",
    "                    #print(\"EPUB Name:\", file)\n",
    "                    \n",
    "                    ebook_info= get_epub_info(epub_path)\n",
    "                    if ebook_info:\n",
    "                        ebook_title=ebook_info['title']\n",
    "                        print('----> working on '+ebook_title)\n",
    "                        ebook_name_clean = re.sub(r'\\(.*?\\)', '', ebook_title)\n",
    "                        if ':' in ebook_name_clean:\n",
    "                            ebook_name_clean = ebook_name_clean.split(':')[0]\n",
    "                        ebook_creator=ebook_info['creator']\n",
    "                        new_creator_name=ebook_creator\n",
    "                        if ',' in ebook_creator:\n",
    "                            creator_names= ebook_creator.split(',')\n",
    "                            first_name=creator_names[1].strip()\n",
    "                            second_name=creator_names[0].strip()\n",
    "                            new_creator_name=first_name+' '+second_name\n",
    "                            #print(new_creator_name)\n",
    "                        bookname=ebook_name_clean+' by '+new_creator_name\n",
    "                        process_book_search(bookname, book_url_local, new_creator_name, current_csv, list_of_user_agents,epub_path) \n",
    "                        #print(bookname)\n",
    "                    else:\n",
    "                        print(f\"{book_url_local} info corrupted.. skipping.\")\n",
    "                        pass\n",
    "                else:\n",
    "                    print(f\"{book_url_local} is already in the CSV file.\")\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file1 = 'authorsoceanofpdf.csv'\n",
    "csv_file = csv.reader(open('aid1.csv', \"r\", encoding='utf-8'), delimiter=\",\")\n",
    "next(csv_file)\n",
    "for row in csv_file:\n",
    "    if row:\n",
    "        a_id = row[0].strip()\n",
    "        if not is_author_id_in_csv(a_id) and not is_author_id_in_local_csv(a_id, csv_file1):\n",
    "            print('========> adding author id '+a_id)\n",
    "            author_data = scrape_author(a_id)  # Scrape author data\n",
    "            append_author_to_csv('authorsoceanofpdf.csv', author_data)\n",
    "            print(f\"+++++++++> {author_data['author_name']} added\")\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            print(f\"{a_id} is already in the CSV file.\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
