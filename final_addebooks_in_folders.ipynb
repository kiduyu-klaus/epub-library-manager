{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Regular expression operations\n",
    "import time  # Time-related functions\n",
    "from urllib.request import urlopen  # URL opening function\n",
    "from bs4 import BeautifulSoup  # HTML parsing library\n",
    "import bs4  # Beautiful Soup library\n",
    "import requests  # HTTP library\n",
    "from pathlib import Path  # File system path operations\n",
    "import pandas as pd  # Data analysis and manipulation library\n",
    "import urllib.request as urllib  # URL request library\n",
    "from matplotlib.pyplot import title  # Matplotlib plotting library\n",
    "import requests  # HTTP library\n",
    "import urllib  # URL library\n",
    "import pandas as pd  # Data analysis and manipulation library\n",
    "from requests_html import HTML  # HTML parsing library\n",
    "from requests_html import HTMLSession  # HTML session library\n",
    "import os  # Operating system library\n",
    "import glob\n",
    "from os.path import exists  # File existence checking\n",
    "import json  # JSON manipulation library\n",
    "from datetime import datetime  # Date and time library\n",
    "import shutup  # Custom library (not recognized by standard Python)\n",
    "import csv  # CSV file operations library\n",
    "import sys  # System-specific parameters and functions\n",
    "from csv import writer  # CSV file writing library\n",
    "import random  # Random number generation library\n",
    "from requests_html import HTML  # HTML parsing library\n",
    "from requests_html import HTMLSession  # HTML session library\n",
    "import ebooklib  # E-book manipulation library\n",
    "from ebooklib import epub  # EPUB file manipulation library\n",
    "import epub_meta  # EPUB metadata extraction library\n",
    "import zipfile  # ZIP file manipulation library\n",
    "from lxml import etree  # XML processing library\n",
    "import click  # Command line interface library\n",
    "# Concurrent programming library\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading  # Thread-based programming library\n",
    "from langdetect import detect  # Language detection library\n",
    "import xml.etree.ElementTree as ET  # XML parsing library\n",
    "from urllib.request import urlopen, Request\n",
    "# Importing UserAgent from the fake_useragent library for generating fake user agents\n",
    "from fake_useragent import UserAgent\n",
    "# Importing the unescape function from the html module for unescaping HTML entities\n",
    "from html import unescape\n",
    "import subprocess   # Importing the subprocess module for managing subprocesses\n",
    "from tqdm import tqdm   # Importing tqdm for creating progress bars\n",
    "import socket   # Importing the socket module for low-level networking operations\n",
    "# Importing the get function from the requests library for making HTTP requests\n",
    "from requests import get\n",
    "# Importing the LibgenSearch class from the libgen_api library for interacting with the Library Genesis API\n",
    "from libgen_api import LibgenSearch\n",
    "from os import remove  # Importing the remove function from the os module for deleting files\n",
    "# Setting filters for book titles: language and extension\n",
    "title_filters = {\"Language\": \"English\", \"Extension\": \"epub\"}\n",
    "tf = LibgenSearch()  # Create an instance of the LibgenSearch class for interacting with the Library Genesis (Libgen) database\n",
    "noimage = \"https://t3.ftcdn.net/jpg/04/34/72/82/360_F_434728286_OWQQvAFoXZLdGHlObozsolNeuSxhpr84.jpg\"  # Default URL for the cover image if retrieval fails\n",
    "if sys.version_info >= (3,):   # Checking if the Python version is 3 or higher\n",
    "    # Importing urllib.request as urllib2 for Python 3\n",
    "    import urllib.request as urllib2\n",
    "    import urllib.parse as urlparse   # Importing urllib.parse as urlparse for Python 3\n",
    "else:\n",
    "    import urllib2   # Importing urllib2 for Python 2\n",
    "    import urlparse   # Importing urlparse for Python 2\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen, HTTPError\n",
    "# Setting the maximum column width for pandas DataFrame to None, allowing unlimited width\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "import subprocess  # Import the subprocess module for executing external commands\n",
    "\n",
    "# Setting the URL for the subcategory image\n",
    "sub_cat_image = 'https://cdn-icons-png.flaticon.com/512/2702/2702069.png'\n",
    "# Setting the URL for the category image\n",
    "cat_image = 'https://cdn.iconscout.com/icon/free/png-512/free-ebook-1473378-1251457.png'\n",
    "\n",
    "status = 1   # Setting the status variable to 1\n",
    "show_on_home = 1   # Setting the show_on_home variable to 1\n",
    "\n",
    "# Setting the URL for the Goodreads website\n",
    "GOODREADS_URL = \"https://www.goodreads.com\"\n",
    "from send2trash import send2trash\n",
    "info = {}   # Initializing an empty dictionary called info\n",
    "\n",
    "EBOOK_HUNTER_URL=\"https://ebook-hunter.org\" # Setting the URL for the ebook-hunter website\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import quote_plus\n",
    "list_of_user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "            'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "            'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "            'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    "             \n",
    "            ]\n",
    "import os\n",
    "import fnmatch\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(category=urllib3.exceptions.InsecureRequestWarning)\n",
    "from googlesearch import search\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import shutil \n",
    "import mysql.connector\n",
    "\n",
    "import wikipedia\n",
    "from wikipedia.exceptions import PageError, DisambiguationError\n",
    "\n",
    "from libgen_api_enhanced import LibgenSearch\n",
    "s = LibgenSearch()\n",
    "\n",
    "from urllib.parse import urlsplit, urlunsplit ,urlparse, parse_qs\n",
    "\n",
    "    \n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "Gemini_key = os.getenv('GEMINI_KEY')\n",
    "from selenium.webdriver import Edge\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions  # For newer versions (selenium 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85e012",
   "metadata": {},
   "source": [
    "### All Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_url_column(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Replace the specified part in the 'url' column\n",
    "    df['url'] = df['url'].str.replace(r'^/content/drive/MyDrive/genre_codes/', '', regex=True)\n",
    "\n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "def replace_uploads_path(input_csv, output_csv, column_name='url'):\n",
    "    \"\"\"\n",
    "    Replace 'uploads\\\\' with 'upload\\\\' in the 'url' column of a CSV file and save the result.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to save the modified CSV file.\n",
    "        column_name (str): Name of the column containing file paths (default: 'url').\n",
    "\n",
    "    Returns:\n",
    "        None (saves the modified DataFrame to output_csv)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the input CSV\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        # Check if the column exists\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' not found in the CSV.\")\n",
    "\n",
    "        # Replace 'uploads\\' with 'upload\\'\n",
    "        df[column_name] = df[column_name].str.replace('uploads\\\\', 'upload\\\\', regex=False)\n",
    "\n",
    "        # Save the modified DataFrame to output CSV\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Successfully saved modified data to: {output_csv}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_csv}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def find_book_by_file_url_in_csv(csv_file_path, file_url):\n",
    "    \"\"\"\n",
    "    Search for a book by its file_url in a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path (str): Path to the CSV file.\n",
    "        file_url (str): The book_file_url to search for.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of matching rows (empty if no match).\n",
    "    \"\"\"\n",
    "    matching_rows = []\n",
    "    try:\n",
    "        with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if row['book_file_url'] == file_url:\n",
    "                    matching_rows.append(row)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{csv_file_path}' not found.\")\n",
    "    except KeyError:\n",
    "        print(\"Error: 'book_file_url' column not found in the CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return matching_rows\n",
    "\n",
    "def is_author_id_in_old_csv(aid, csv_file='tbl_author_old.csv'):\n",
    "    try:\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if row['author_id'] == str(aid):  # Check if the 'author_id' column matches the author ID\n",
    "                    return row\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {csv_file} not found.\")\n",
    "        return None\n",
    "\n",
    "def match_author(matching_row):\n",
    "    \"\"\"\n",
    "    Match the author data from the CSV file.\n",
    "\n",
    "    Args:\n",
    "        matching_row (dict): A dictionary containing the matched row from the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the matched author data.\n",
    "    \"\"\"\n",
    "    author_id = matching_row['author_id']\n",
    "    author_name = matching_row['author_name']\n",
    "    author_city_name = matching_row['author_city_name']\n",
    "    author_description = matching_row['author_description']\n",
    "    author_image = matching_row['author_image']\n",
    "    author_youtube = matching_row['author_youtube']\n",
    "    author_instagram = matching_row['author_instagram']\n",
    "    author_facebook = matching_row['author_facebook']\n",
    "    author_website = matching_row['author_website']\n",
    "\n",
    "    return {\n",
    "        \"id\": author_id,\n",
    "        \"name\": author_name,\n",
    "        \"description\": author_description,\n",
    "        \"image\": author_image,\n",
    "        \"facebook_url\": author_facebook,\n",
    "        \"instagram_url\": author_instagram,\n",
    "        \"youtube_url\": author_youtube,\n",
    "        \"website_url\": author_website,\n",
    "        \"status\": '1'\n",
    "    }\n",
    "    \n",
    "def add_books_to_uploads(book_data,file_path,current_csv):\n",
    "    csv_file1 = 'authorsoceanofpdf.csv'\n",
    "    author_name_id=book_data['author_ids']\n",
    "    book_id=book_data['id']\n",
    "    book_url_local=book_data['url']\n",
    "    book_name=book_data['title']\n",
    "    author_name_row = is_author_id_in_old_csv(author_name_id)\n",
    "    author_name = 'Unknown Authors'\n",
    "    if author_name_row:\n",
    "        author_info= match_author(author_name_row)\n",
    "        author_name = author_info['name']\n",
    "        if not is_author_id_in_csv(author_name_id) and not is_author_id_in_local_csv(author_name_id, csv_file1):\n",
    "            append_author_to_csv('authorsoceanofpdf.csv', author_info)\n",
    "            print(f\"+++++++++> {author_info['name']} added\")\n",
    "        else:\n",
    "            author_name = author_info['name']\n",
    "            print(f\"+++++++++> {author_info['name']} already exists\")\n",
    "    else:\n",
    "        author_data = scrape_author(author_name_id)\n",
    "        for attempt in range(3):  # Try scraping the author up to 3 times\n",
    "            author_data = scrape_author(author_name_id)\n",
    "            if author_data:\n",
    "                author_name = author_data['name']\n",
    "                if not is_author_id_in_csv(author_name_id) and not is_author_id_in_local_csv(author_name_id, csv_file1):\n",
    "                    append_author_to_csv('authorsoceanofpdf.csv', author_data)\n",
    "                    print(f\"+++++++++> {author_data['name']} added\")\n",
    "                else:\n",
    "                    author_name = author_data['name']\n",
    "                    print(f\"+++++++++> {author_data['name']} already exists\")\n",
    "                break  # Exit the loop if author_data is found\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}: Failed to scrape author data for ID {author_name_id}\")\n",
    "                \n",
    "                \n",
    "    \n",
    "    if author_name.lower() != 'unknown':\n",
    "        dest = os.path.join('upload', author_name)\n",
    "        if not os.path.exists(dest):\n",
    "            os.makedirs(dest)\n",
    "    if not search_book_id(book_id) and not check_book_id(book_id, current_csv):       \n",
    "        new_file_path=move_and_rename_epub(file_path, dest, book_url_local)\n",
    "        book_data['url'] = new_file_path\n",
    "                        #print('Book file moved to ' + new_file_path)\n",
    "        append_to_book_csv(book_data, current_csv)\n",
    "        print('=========================> Scraping ' + book_name + ' done')\n",
    "    else:\n",
    "        print(f\"{book_data['title']} already added.....skipping.\")\n",
    "        try:\n",
    "            send2trash(file_path)\n",
    "            print(f\"File moved to Recycle Bin: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        print('book deleted')\n",
    "\n",
    "def match_book(matching_rows):\n",
    "    id=matching_rows[0]['id']\n",
    "    cat_id=matching_rows[0]['cat_id']\n",
    "    sub_cat_id=matching_rows[0]['sub_cat_id']\n",
    "    author_ids=matching_rows[0]['aid']\n",
    "    book_access='Free'\n",
    "    featured=matching_rows[0]['featured']\n",
    "    title=matching_rows[0]['book_title']\n",
    "    description=matching_rows[0]['book_description']\n",
    "    image=matching_rows[0]['book_cover_img']\n",
    "    url_type='local'\n",
    "    url=matching_rows[0]['book_file_url']\n",
    "    download_enable='0'\n",
    "    book_on_rent='0'    \n",
    "    \n",
    "    return {\n",
    "                'id': id,\n",
    "                'cat_id': cat_id,\n",
    "                'sub_cat_id': sub_cat_id,\n",
    "                'author_ids': author_ids,\n",
    "                'book_access': 'Free',\n",
    "                'featured': '1',\n",
    "                'title': title,\n",
    "                'description': description,\n",
    "                'image': image,\n",
    "                'url_type': 'local',\n",
    "                'url': url,\n",
    "                'download_enable': '0',\n",
    "                'book_on_rent': '0',\n",
    "                'book_rent_price': None,  # MySQL NULL equivalent in Python\n",
    "                'book_rent_time': None,  # MySQL NULL equivalent in Python\n",
    "                'featured': '0',  \n",
    "                'status': '1'\n",
    "            }\n",
    "    \n",
    "def extract_author(file_path):\n",
    "    # Get the file name from the full path\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # Try to find an author pattern after the last underscore and before .epub\n",
    "    match = re.search(r'_-_([A-Za-z\\._\\s]+)(?:_\\d+)?\\.epub$', filename)\n",
    "    if match:\n",
    "        # Replace underscores and dots with spaces, strip extra whitespace\n",
    "        author = match.group(1).replace('_', ' ').replace('.', ' ').strip()\n",
    "        # Capitalize each word\n",
    "        return ' '.join(word.capitalize() for word in author.split())\n",
    "    else:\n",
    "        return None  # or return 'Unknown'\n",
    "\n",
    "def clean_url_column(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Modify the 'sub_category_image' column to include the 'upload/' prefix\n",
    "    df['sub_cat_image'] = df['sub_cat_image'].apply(\n",
    "        lambda x: f\"upload/{x}\" if not x.startswith(\"upload/\") else x\n",
    "    )\n",
    "\n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Successfully saved modified data to: {output_csv}\")\n",
    "\n",
    "def clean_url_column1(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Replace 'uploads\\' with 'upload\\' in the 'url' column\n",
    "    df['url'] = df['url'].str.replace('uploads\\\\', 'upload\\\\', regex=False)\n",
    "\n",
    "    # Set 'url_type' column to \"local\" for all records\n",
    "    df['url_type'] = 'local'\n",
    "\n",
    "    # Set 'download_enable' to 0 for all records\n",
    "    df['download_enable'] = 0\n",
    "\n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Successfully saved modified data to: {output_csv}\")\n",
    "    \n",
    "def count_rows_in_csv(file_path):\n",
    "    \"\"\"\n",
    "    Counts the number of rows in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of rows in the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return len(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return 0\n",
    "\n",
    "def is_author_id_in_csv(aid, csv_file= 'tbl_author.csv'):\n",
    "    try:\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                if row[0] == str(aid):  # Check if the first column matches the author ID\n",
    "                    return True\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {csv_file} not found.\")\n",
    "        return False\n",
    "\n",
    "def is_author_id_in_local_csv(aid, csv_file):\n",
    "    \"\"\"\n",
    "    Checks if a given author ID exists in the specified CSV file.\n",
    "\n",
    "    Args:\n",
    "        aid (int or str): The author ID to search for.\n",
    "        csv_file (str): The path to the CSV file where the author ID is stored.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the author ID is found in the CSV file, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the CSV file in read mode, specifying UTF-8 encoding for compatibility\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)  # Create a CSV reader object to read rows from the file\n",
    "            for row in reader:\n",
    "                # Check if the first column matches the provided author ID\n",
    "                if row[0] == str(aid):  # Convert 'aid' to string for comparison\n",
    "                    return True  # Return True if a matching author ID is found\n",
    "        return False  # Return False if the author ID is not found after reading all rows\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the CSV file does not exist\n",
    "        print(f\"File {csv_file} not found.\")\n",
    "        return False\n",
    "\n",
    "def get_author_description(soup, id_number):\n",
    "    cell = soup.find(\"span\", {\"id\": \"freeTextContainerauthor\" + id_number})\n",
    "    if cell:\n",
    "        return cell.text.strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_book_description(soup):\n",
    "    \"\"\"\n",
    "    Retrieves the book description from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        str: The book description.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize an empty string for the book description\n",
    "    bdescription = ''\n",
    "    try:\n",
    "        # Check if the book description element exists\n",
    "        if soup.find(\"div\", {\"class\": \"DetailsLayoutRightParagraph__widthConstrained\"}).get_text():\n",
    "            # Retrieve the book description text\n",
    "            bdescription = soup.find(\n",
    "                \"div\", {\"class\": \"DetailsLayoutRightParagraph__widthConstrained\"}).get_text()\n",
    "            # Return the book description\n",
    "            return bdescription\n",
    "    except AttributeError:\n",
    "        # Return 'No Description available' if there is an AttributeError (element not found)\n",
    "        return 'No Description available'\n",
    "\n",
    "    # Return 'No Description available' if the book description is empty\n",
    "    return 'No Description available'\n",
    "\n",
    "def get_id_number(author_id):\n",
    "    \"\"\"\n",
    "    Extracts the ID number from an author ID.\n",
    "\n",
    "    Args:\n",
    "        author_id (str): The author ID.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted ID number.\n",
    "\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"([^.-]+)\")\n",
    "    aid = pattern.search(author_id).group()\n",
    "    author_split = aid.split(\".\")\n",
    "    author_url1 = author_split[0]\n",
    "    return author_url1\n",
    "\n",
    "def get_author_image(soup, author_name):\n",
    "    \"\"\"\n",
    "    Extracts the image URL of an author from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "        author_name (str): The name of the author used to locate the image.\n",
    "\n",
    "    Returns:\n",
    "        str: The URL of the author image if found, or a placeholder image URL otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "    cell = soup.find(\"img\", {\"alt\": author_name, \"itemprop\": \"image\"})\n",
    "    if cell:\n",
    "        return cell.attrs.get(\"src\")\n",
    "    return 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/No-Image-Placeholder.svg/1665px-No-Image-Placeholder.svg.png'\n",
    "\n",
    "def get_author_info(soup):\n",
    "    \"\"\"Get all information from an author (genres, influences, website etc.).\n",
    "    Args:\n",
    "        soup (bs4.element.Tag): author page connection.\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    container = soup.find('div', attrs={'class': 'rightContainer'})\n",
    "    author_info = {}\n",
    "    data_div = container.find('br', attrs={'class': 'clear'})\n",
    "    while data_div:\n",
    "        if data_div.name:\n",
    "            data_class = data_div.get('class')[0]\n",
    "            # Information section is finished\n",
    "            if data_class == 'aboutAuthorInfo':\n",
    "                break\n",
    "            # Key elements\n",
    "            elif data_class == 'dataTitle':\n",
    "                key = data_div.text.strip()\n",
    "                author_info[key] = []\n",
    "            # Born section\n",
    "            if data_div.text == 'Born':\n",
    "                data_div = data_div.next_sibling\n",
    "                author_info[key].append(data_div.strip())\n",
    "            # Influences section\n",
    "            elif data_div.text == 'Influences':\n",
    "                data_div = data_div.next_sibling.next_sibling\n",
    "                data_items = data_div.find_all('span')[-1].find_all('a')\n",
    "                for data_a in data_items:\n",
    "                    author_info[key].append(data_a.text.strip())\n",
    "            # Member since section\n",
    "            elif data_div.text == 'Member Since':\n",
    "                data_div = data_div.next_sibling.next_sibling\n",
    "                author_info[key].append(data_div.text.strip())\n",
    "            # Genre, website and other sections\n",
    "            else:\n",
    "                data_items = data_div.find_all('a')\n",
    "                for data_a in data_items:\n",
    "                    author_info[key].append(data_a.text.strip())\n",
    "        data_div = data_div.next_sibling\n",
    "    return author_info\n",
    "\n",
    "def author_youtube_search(Author_name):\n",
    "    \"\"\"\n",
    "    Search for the author's YouTube channel using the specified author's name.\n",
    "    \n",
    "    Args:\n",
    "        Author_name (str): The author's name.\n",
    "        \n",
    "    Returns:\n",
    "        str: The YouTube channel link if found, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append the author's name and additional keywords to the search query\n",
    "        search_txt = Author_name + ' channel youtube'\n",
    "        \n",
    "        # Perform the search with a limit of 10 results\n",
    "        search_results = search(search_txt, num_results=10)\n",
    "        \n",
    "        # Look for a YouTube link in the search results\n",
    "        for result in search_results:\n",
    "            if 'https://www.youtube.com' in result:\n",
    "                return result\n",
    "        \n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    # Return an empty string if no YouTube link is found or an error occurred\n",
    "    return ''\n",
    "\n",
    "def author_instagram_search(Author_name):\n",
    "    \"\"\"\n",
    "    Search for the author's official Instagram account using the specified author's name.\n",
    "    \n",
    "    Args:\n",
    "        Author_name (str): The author's name.\n",
    "        \n",
    "    Returns:\n",
    "        str: The Instagram account link if found, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append the author's name and additional keywords to the search query\n",
    "        search_txt = Author_name + ' instagram official'\n",
    "        \n",
    "        # Perform the search with a limit of 10 results\n",
    "        search_results = search(search_txt, num_results=10)\n",
    "        \n",
    "        # Look for an Instagram link in the search results\n",
    "        for result in search_results:\n",
    "            if 'https://www.instagram.com' in result:\n",
    "                return result\n",
    "    \n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    # Return an empty string if no Instagram link is found or an error occurred\n",
    "    return ''\n",
    "\n",
    "def author_facebook_search(Author_name):\n",
    "    \"\"\"\n",
    "    Search for the author's official Facebook page using the specified author's name.\n",
    "    \n",
    "    Args:\n",
    "        Author_name (str): The author's name.\n",
    "        \n",
    "    Returns:\n",
    "        str: The Facebook page link if found, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append the author's name and additional keywords to the search query\n",
    "        search_txt = Author_name + ' facebook official'\n",
    "        \n",
    "        # Perform the search with a limit of 10 results\n",
    "        search_results = search(search_txt, num_results=10)\n",
    "        \n",
    "        # Look for a Facebook link in the search results\n",
    "        for result in search_results:\n",
    "            if 'https://www.facebook.com' in result:\n",
    "                return result\n",
    "\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError occurred while searching Facebook: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while searching Facebook: {e}\")\n",
    "    \n",
    "    # Return an empty string if no Facebook link is found or an error occurred\n",
    "    return ''\n",
    "\n",
    "def author_website_search(author_name):\n",
    "    \"\"\"\n",
    "    Search for the author's official website using the specified author's name.\n",
    "    \n",
    "    Args:\n",
    "        author_name (str): The author's name.\n",
    "        \n",
    "    Returns:\n",
    "        str: The website link if found, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append the author's name and additional keywords to the search query\n",
    "        search_txt = author_name + ' official website'\n",
    "        \n",
    "        # Perform the search with a limit of 10 results\n",
    "        search_results = search(search_txt, num_results=10)\n",
    "        \n",
    "        # Return the first valid result\n",
    "        for result in search_results:\n",
    "            return result\n",
    "\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError occurred while searching for the website: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while searching for the website: {e}\")\n",
    "    \n",
    "    # Return an empty string if no website link is found or an error occurred\n",
    "    return ''\n",
    "\n",
    "def scrape_author(author_id):\n",
    "        \"\"\"\n",
    "        Scrapes the author information from the Goodreads website.\n",
    "\n",
    "        Args:\n",
    "            author_id (str): The author ID.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the scraped author information.\n",
    "        \"\"\"\n",
    "        user_agent = random.choice(list_of_user_agents)  # Select a random user agent\n",
    "        random_header = {'User-Agent': user_agent}  # Create a header with the selected user agent\n",
    "\n",
    "        url = \"https://www.goodreads.com/author/show/\" + author_id\n",
    "\n",
    "        time.sleep(3)  # Pause execution for 3 seconds\n",
    "\n",
    "        try:\n",
    "            source = requests.get(url, headers=random_header).content  # Open the URL and retrieve the HTML source with the header\n",
    "            soup = BeautifulSoup(source, \"html.parser\")  # Create a BeautifulSoup object for parsing the HTML\n",
    "\n",
    "            author_name = soup.find(\"span\", {\"itemprop\": \"name\"}).text.strip()  # Extract the author name from the HTML\n",
    "            id_number = get_id_number(author_id)  # Call a helper function to get the ID number\n",
    "\n",
    "            author_info = get_author_info(soup)  # Call a helper function to get additional author information\n",
    "            if author_info:\n",
    "                if 'Born' in author_info:\n",
    "                    author_city_name = author_info[\"Born\"][0]  # Extract the author's city name if available\n",
    "                else:\n",
    "                    author_city_name = 'No City Name Found'\n",
    "\n",
    "                if 'Website' in author_info:\n",
    "                    author_website = author_info[\"Website\"]  # Extract the author's website if available\n",
    "                else:\n",
    "                    author_website = 'none'\n",
    "            else:\n",
    "                author_city_name = 'No City Name Found'\n",
    "                author_website = 'none'\n",
    "\n",
    "            #info[\"author_name\"] = author_name  # Add the author name to the 'info' dictionary\n",
    "\n",
    "            author_des = get_author_description(soup, id_number)\n",
    "            if not author_des:  # Check if author_des is empty (None or empty string)\n",
    "                author_des = \"No Author Description\"\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(f\"An AttributeError occurred while scraping author information: {e}\")\n",
    "            return None\n",
    "     \n",
    "        #\"id\",\"name\",\"description\",\"image\",\"facebook_url\",\"instagram_url\",\"youtube_url\",\"website_url\",\"status\"\n",
    "        return {\n",
    "            \"id\": id_number,\n",
    "            \"name\": author_name,\n",
    "            \"description\": author_des,\n",
    "            \"image\": get_author_image(soup, author_name),\n",
    "            \"facebook_url\": author_facebook_search(author_name),\n",
    "            \"instagram_url\": author_instagram_search(author_name),\n",
    "            \"youtube_url\": author_youtube_search(author_name),\n",
    "            \"website_url\": author_website_search(author_name),\n",
    "            \"status\": '1'\n",
    "        }\n",
    "        \n",
    "def append_author_to_csv(file_name, author_data):\n",
    "    \"\"\"\n",
    "    Appends the scraped author data to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the CSV file.\n",
    "        author_data (dict): A dictionary containing the scraped author information.\n",
    "    \"\"\"\n",
    "    fieldnames = [\n",
    "        'id', 'name', 'description', 'image',\n",
    "        'facebook_url', 'instagram_url', 'youtube_url',\n",
    "        'website_url', 'status'\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Check if the file exists and write header if not\n",
    "    try:\n",
    "        with open(file_name, mode='r', newline='', encoding='utf-8') as file:\n",
    "            pass\n",
    "    except FileNotFoundError:\n",
    "        with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "    # Append author data to the CSV file\n",
    "    with open(file_name, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writerow(author_data)\n",
    "\n",
    "def book_google_search(b_name):\n",
    "    \"\"\"\n",
    "    Search for the author's YouTube channel using the specified author's name.\n",
    "    \n",
    "    Args:\n",
    "        Author_name (str): The author's name.\n",
    "        \n",
    "    Returns:\n",
    "        str: The YouTube channel link if found, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append the author's name and additional keywords to the search query\n",
    "        search_txt = b_name + ' goodreads book show'\n",
    "        \n",
    "        # Perform the search with a limit of 10 results\n",
    "        search_results = search(search_txt, num_results=10)\n",
    "        \n",
    "        # Look for a YouTube link in the search results\n",
    "        for result in search_results:\n",
    "            #print(result)\n",
    "            if result.startswith('https://www.goodreads.com/book/show/') or result.startswith('https://www.goodreads.com/en/book/show'):\n",
    "                return result\n",
    "        \n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    # Return an empty string if no YouTube link is found or an error occurred\n",
    "    return ''\n",
    "\n",
    "# Configure Gemini AI with the API key from the environment variable\n",
    "genai.configure(api_key=Gemini_key)\n",
    "\n",
    "# Create the model configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 0.7,  # Lower temperature for deterministic responses\n",
    "    \"top_p\": 0.95,  # Use nucleus sampling\n",
    "    \"top_k\": 40,  # Consider top-k tokens\n",
    "    \"max_output_tokens\": 512,  # Limit response length\n",
    "    \"response_mime_type\": \"text/plain\",  # Expect text response\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash-exp\",  # Use the appropriate model name\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "def get_goodreads_title(book_title):\n",
    "    \"\"\"\n",
    "    Passes a book title and author to Gemini AI to get the Goodreads official book link.\n",
    "\n",
    "    Args:\n",
    "        book_title (str): The title of the book.\n",
    "        author_name (str): The name of the author.\n",
    "\n",
    "    Returns:\n",
    "        str: The Goodreads official book link or an error message.\n",
    "    \"\"\"\n",
    "    # Start a new chat session\n",
    "    chat_session = model.start_chat(history=[])\n",
    "\n",
    "    # Construct the input prompt\n",
    "    prompt = (\n",
    "        f\"Find the official Goodreads title for the book titled '{book_title}' \"\n",
    "        f\" Return only the goodreads title and Author.\"\n",
    "    )\n",
    "\n",
    "    # Send the message to the Gemini model\n",
    "    response = chat_session.send_message(prompt)\n",
    "\n",
    "    # Extract the text response\n",
    "    response_text = response.text.strip()\n",
    "    if response_text.strip().lower() == book_title.strip().lower():\n",
    "        return None\n",
    "    return response_text\n",
    "\n",
    "    # Validate the output (basic validation for Goodreads link)\n",
    "    #if response_text.startswith(\"https://www.goodreads.com\"):\n",
    "        #return response_text\n",
    "    #else:\n",
    "        #return \"Could not retrieve a valid Goodreads link.\"\n",
    "\n",
    "def is_book_in_csv(book_file_url, csv_file):\n",
    "    \"\"\"\n",
    "    Checks if a book with the given file URL is present in the specified CSV file.\n",
    "    \n",
    "    Args:\n",
    "        book_file_url (str): The file URL of the book to search for.\n",
    "        csv_file (str): The path to the CSV file.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the book is found in the CSV file, otherwise False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Check if the 'book_file_url' column contains the specified URL\n",
    "        if 'url' in df.columns:\n",
    "            if book_file_url in df['url'].values:\n",
    "                return True\n",
    "        else:\n",
    "            print(\"Error: The CSV file does not contain a 'book_file_url' column.\")\n",
    "            return False\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_file} does not exist.\")\n",
    "        return False\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: The file {csv_file} is empty.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "def split_epub_filename(epub_filename):\n",
    "    \"\"\"\n",
    "    Splits an EPUB filename into title and author.\n",
    "    \n",
    "    Args:\n",
    "        epub_filename (str): The EPUB filename.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the 'title' and 'author' as separate strings.\n",
    "    \"\"\"\n",
    "    # Remove the file extension (.epub)\n",
    "    filename = os.path.splitext(epub_filename)[0]\n",
    "    \n",
    "    # Find the last hyphen to split title and author\n",
    "    last_hyphen_index = filename.rfind(' - ')\n",
    "    \n",
    "    if last_hyphen_index != -1:\n",
    "        title = filename[:last_hyphen_index].strip()\n",
    "        author = filename[last_hyphen_index + 3:].strip()  # Skip past the hyphen and spaces\n",
    "    else:\n",
    "        # If no hyphen is found, treat the whole filename as the title\n",
    "        title = filename.strip()\n",
    "        author = 'Unknown'\n",
    "\n",
    "    # Remove the author's name from the title if it appears in it\n",
    "    title_without_author = title.replace(author, '').strip()\n",
    "    \n",
    "    pattern = r'\\b\\d+\\s*(?=\\w)'\n",
    "\n",
    "    # Remove any leading character or number before the title\n",
    "    title_cleaned = re.sub(r'^[^A-Za-z0-9]*(.*)', r'\\1', title_without_author).strip()\n",
    "    title_cleaned1 = re.sub(pattern, '',  title_cleaned).strip()\n",
    "    title_cleaned2 = re.sub(r'^[\\d.]+\\s*', '', title_cleaned1).strip()\n",
    "\n",
    "    return {\n",
    "        'title': title_cleaned2,\n",
    "        'author': author\n",
    "    }\n",
    "\n",
    "def clean_ebook_filename11(filename):\n",
    "    \"\"\"\n",
    "    Removes specified substrings from an ebook filename.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The original ebook filename.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned ebook filename.\n",
    "    \"\"\"\n",
    "    # Remove everything between ( and ) including the parentheses\n",
    "    cleaned_filename = re.sub(r'\\s*\\(.*?\\)\\s*', '', filename)\n",
    "    \n",
    "    \n",
    "    cleaned_filename = re.sub(r'^[\\d.]+\\s*', '', filename)\n",
    "\n",
    "    # Remove everything after just ( including it\n",
    "    cleaned_filename = re.sub(r'\\s*\\(.*', '', cleaned_filename)\n",
    "\n",
    "    # Remove everything after the first - or _ (including them)\n",
    "    cleaned_filename = re.sub(r'\\s*[-_].*', '', cleaned_filename)\n",
    "\n",
    "    # Strip any extra spaces\n",
    "    return cleaned_filename.strip()\n",
    "\n",
    "def clean_ebook_filename(text):\n",
    "    return re.sub(r'\\s*\\([^)]*\\)', '', text).strip()\n",
    "\n",
    "def do_google_search(query, num_results, driver):\n",
    "    querys=quote_plus(query)\n",
    "    g_search_q=f\"https://www.google.com/search?q={querys}&num={num_results}\"\n",
    "    # Perform a Google search with the specified query and number of results\n",
    "    driver.get(g_search_q)\n",
    "    WebDriverWait(driver, 5).until(ec.presence_of_element_located((By.ID, 'main')))\n",
    "    #time.sleep(5)  # Allow more time for all results to load\n",
    "    print(g_search_q)\n",
    "    links = driver.find_elements(By.CSS_SELECTOR, 'div.yuRUbf a')\n",
    "    urls = [link.get_attribute(\"href\") for link in links if link.get_attribute(\"href\") and 'http' in link.get_attribute(\"href\")]\n",
    "    urls = [url for url in urls if 'translate.google.com' not in url]  # Exclude Google Translate links\n",
    "\n",
    "    unique_domains = set()\n",
    "    unique_urls = []\n",
    "    for url in urls:\n",
    "        domain = urlparse(url).netloc\n",
    "        if domain not in unique_domains:\n",
    "            unique_domains.add(domain)\n",
    "            unique_urls.append(url)\n",
    "\n",
    "    return urls\n",
    "\n",
    "def g_search(text):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    #chrome_options.add_argument(\"--headless\")  # Ensure GUI is off\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    # Pick a random user-agent\n",
    "    user_agent = random.choice(list_of_user_agents)\n",
    "    chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    urls = do_google_search(text, 5, driver)\n",
    "    print(f'Found {len(urls)} URLs:')\n",
    "    for url in urls:\n",
    "        if url.startswith('https://www.goodreads.com/book/show/') or url.startswith('https://www.goodreads.com/en/book/show'):\n",
    "            print(url)\n",
    "            return url\n",
    "            \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "   \n",
    "def get_genre_list(soup):\n",
    "    \"\"\"\n",
    "    Extracts genre information from a BeautifulSoup object representing an HTML page.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries representing genres with their corresponding 'sid' and 'found_cat_id'.\n",
    "    \"\"\"\n",
    "\n",
    "    genres = []\n",
    "    genres_container = soup.find(\"div\", {\"data-testid\": \"genresList\"})\n",
    "\n",
    "    if genres_container:\n",
    "        genre_links = genres_container.find(\"ul\").find(\"span\").find_all(\"a\")\n",
    "\n",
    "        for link in genre_links:\n",
    "            genre = link.find(\"span\").text\n",
    "            genres.append(genre)\n",
    "        categories_string = ', '.join(genres)\n",
    "        return categories_string\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_id(bookid):\n",
    "    \"\"\"\n",
    "    Extracts the ID from a book ID.\n",
    "\n",
    "    Args:\n",
    "        bookid (str): The book ID.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted ID.\n",
    "\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"([^.-]+)\")\n",
    "    bookdd = bookid.split('-')[0]\n",
    "    bookdd = bookdd.split('.')[0]\n",
    "    return bookdd\n",
    "\n",
    "def get_author_description(soup, id_number):\n",
    "    cell = soup.find(\"span\", {\"id\": \"freeTextContainerauthor\" + id_number})\n",
    "    if cell:\n",
    "        return cell.text.strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_author_id(soup):\n",
    "    \"\"\"\n",
    "    Retrieves one or more author IDs from a Goodreads book page.\n",
    "\n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        str: A single author ID or multiple author IDs separated by commas.\n",
    "    \"\"\"\n",
    "    author_ids = []\n",
    "    contributors_section = soup.find('div', class_='ContributorLinksList')\n",
    "\n",
    "    if contributors_section:\n",
    "        author_links = contributors_section.find_all('a', class_='ContributorLink')\n",
    "        for link in author_links:\n",
    "            author_url = link.get('href', '')\n",
    "            if '/author/show/' in author_url:\n",
    "                author_id = author_url.split('/')[-1].split('.')[0]\n",
    "                author_ids.append(author_id)\n",
    "\n",
    "    return ','.join(author_ids) if author_ids else None\n",
    "\n",
    "def get_book_description(soup):\n",
    "    \"\"\"\n",
    "    Retrieves the book description from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        str: The book description.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize an empty string for the book description\n",
    "    bdescription = ''\n",
    "    try:\n",
    "        # Check if the book description element exists\n",
    "        if soup.find(\"div\", {\"class\": \"DetailsLayoutRightParagraph__widthConstrained\"}).get_text():\n",
    "            # Retrieve the book description text\n",
    "            bdescription = soup.find(\n",
    "                \"div\", {\"class\": \"DetailsLayoutRightParagraph__widthConstrained\"}).get_text()\n",
    "            # Return the book description\n",
    "            return bdescription\n",
    "    except AttributeError:\n",
    "        # Return 'No Description available' if there is an AttributeError (element not found)\n",
    "        return 'No Description available'\n",
    "\n",
    "    # Return 'No Description available' if the book description is empty\n",
    "    return 'No Description available'\n",
    "    \n",
    "def get_sub_category_id(sub_category_name, csv_file='subcategories.csv'):\n",
    "    \"\"\"\n",
    "    Searches for a subcategory in the CSV file and returns its ID and associated category ID.\n",
    "    Exits the search once the subcategory is found.\n",
    "\n",
    "    Args:\n",
    "        sub_category_name (str): The subcategory name to search for.\n",
    "        csv_file (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'sid' and 'cat_id' if found, otherwise None.\n",
    "    \"\"\"\n",
    "    # Normalize the subcategory name\n",
    "    sub_category_name = sub_category_name.strip().lower()\n",
    "\n",
    "    try:\n",
    "        with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "\n",
    "            for row in reader:\n",
    "                # Normalize the subcategory name from the CSV file\n",
    "                csv_sub_category_name = row['sub_cat_name'].strip().lower()\n",
    "\n",
    "                if csv_sub_category_name == sub_category_name:\n",
    "                    return {\n",
    "                        'sid': int(row['sid']),  # Return the subcategory ID as an integer\n",
    "                        'cat_id': int(row['cat_id'])  # Return the category ID as an integer\n",
    "                    }\n",
    "                #print(f\"{sub_category_name} not found in {csv_file}.\")\n",
    "            # Return None if the subcategory is not found\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_file} does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_subcategories(sub_cat_id_string, csv_file='subcategories.csv'):\n",
    "    \"\"\"\n",
    "    Processes the sub_cat_id string from the book data, searches for each subcategory\n",
    "    in the CSV file, and returns the ID and category ID of the first found subcategory.\n",
    "\n",
    "    Args:\n",
    "        sub_cat_id_string (str): Comma-separated string of subcategory names.\n",
    "        csv_file (str): The path to the CSV file containing subcategories.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'sid', 'cat_id', and 'sub_cat_name' of the first found subcategory,\n",
    "              otherwise creates a new subcategory and returns its information.\n",
    "    \"\"\"\n",
    "    if sub_cat_id_string:\n",
    "        subcategories = [sub_cat.strip() for sub_cat in sub_cat_id_string.split(',')]\n",
    "        \n",
    "        # Skip specific genres if there are more than 2 subcategories including them\n",
    "        skip_genres = {'fiction', 'mystery', 'romance'}\n",
    "        filtered_subcategories = []\n",
    "\n",
    "        for genre in skip_genres:\n",
    "            if genre in (sub.lower() for sub in subcategories):\n",
    "                count_including_genre = sum(1 for sub in subcategories if sub.lower() != genre)\n",
    "                if count_including_genre > 1:\n",
    "                    filtered_subcategories = [sub for sub in subcategories if sub.lower() != genre]\n",
    "                else:\n",
    "                    filtered_subcategories = subcategories\n",
    "            else:\n",
    "                filtered_subcategories = subcategories\n",
    "\n",
    "        subcategories = filtered_subcategories\n",
    "\n",
    "        for subcategory in subcategories:\n",
    "            # Search for the subcategory ID\n",
    "            subcategory_info = get_sub_category_id(subcategory, csv_file)\n",
    "            if subcategory_info is not None:\n",
    "                return {\n",
    "                    'sid': subcategory_info['sid'],\n",
    "                    'cat_id': subcategory_info['cat_id'],\n",
    "                    'sub_cat_name': subcategory\n",
    "                }\n",
    "\n",
    "        # If no subcategory is found, create a new one\n",
    "        new_subcategory_name = subcategories[0]\n",
    "\n",
    "        # Safe check before reading\n",
    "        if os.path.exists(csv_file) and os.path.getsize(csv_file) > 0:\n",
    "            df = pd.read_csv(csv_file)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=['sid', 'cat_id', 'sub_cat_name', 'sub_cat_image', 'status'])\n",
    "\n",
    "        new_sid = df['sid'].max() + 1 if not df.empty else 1\n",
    "\n",
    "        new_subcategory = {\n",
    "            'sid': new_sid,\n",
    "            'cat_id': 1,\n",
    "            'sub_cat_name': new_subcategory_name,\n",
    "            'sub_cat_image': new_subcategory_name + '.jpg',\n",
    "            'status': 1\n",
    "        }\n",
    "\n",
    "        new_df = pd.DataFrame([new_subcategory])\n",
    "\n",
    "        # Write header only if file doesn't exist or is empty\n",
    "        write_header = not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0\n",
    "        new_df.to_csv(csv_file, mode='a', index=False, header=write_header)\n",
    "\n",
    "        return {\n",
    "            'sid': new_sid,\n",
    "            'cat_id': 1,\n",
    "            'sub_cat_name': new_subcategory_name\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        return {\n",
    "            'sid': 1,\n",
    "            'cat_id': 1,\n",
    "            'sub_cat_name': 'Default'\n",
    "        }\n",
    "            \n",
    "def get_book_cover(soup):\n",
    "    \"\"\"\n",
    "    Retrieves the book cover image URL from a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup (bs4.BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "        str: The URL of the book cover image.\n",
    "\n",
    "    \"\"\"\n",
    "    if soup.find('img', {'class': 'ResponsiveImage'}):\n",
    "        cover = soup.find('img', {'class': 'ResponsiveImage'})\n",
    "        return cover.attrs.get('src')\n",
    "    #if soup.find(id=\"coverImage\"):\n",
    "        #cover = soup.find(id=\"coverImage\")\n",
    "       # print(cover)\n",
    "       # return cover.get('src')  # img.get\n",
    "    return 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/No-Image-Placeholder.svg/1665px-No-Image-Placeholder.svg.png'\n",
    "\n",
    "def scrape_book(book_url, book_url_local,random_header):\n",
    "    \"\"\"\n",
    "    Scrapes book information from a Goodreads book URL.\n",
    "\n",
    "    Args:\n",
    "        book_url (str): The Goodreads book URL.\n",
    "        book_url_local (str): The local URL of the book file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the scraped book information.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    url = book_url\n",
    "    source = requests.get(url,headers = random_header)\n",
    "    #print(source.status_code)\n",
    "    time.sleep(2)\n",
    "    soup = bs4.BeautifulSoup(source.content, 'html.parser')\n",
    "    book_title_element = soup.find('h1', class_='Text Text__title1', attrs={'data-testid': 'bookTitle'})\n",
    "    book_title =  ' '.join(book_title_element.text.split())\n",
    "    book_id_beta = book_url.replace('https://www.goodreads.com/book/show/', '')\n",
    "    book_id_beta1 = book_id_beta.replace(\n",
    "                'https://www.goodreads.com/en/book/show/', '')\n",
    "    g_list=get_genre_list(soup)\n",
    "    #print(g_list)\n",
    "    category_ids = process_subcategories(g_list)\n",
    "    sid = category_ids.get('sid')\n",
    "    cat_id = category_ids.get('cat_id')\n",
    "    #sid = 41\n",
    "    #cat_id = 3\n",
    "    \n",
    "    return {\n",
    "                'id': get_id(book_id_beta1),\n",
    "                'cat_id': cat_id,\n",
    "                'sub_cat_id': sid,\n",
    "                'author_ids': get_author_id(soup),\n",
    "                'book_access': 'Free',\n",
    "                'featured': '1',\n",
    "                'title': book_title,\n",
    "                'description': get_book_description(soup),\n",
    "                'image': get_book_cover(soup),\n",
    "                'url_type': 'local',\n",
    "                'url': book_url_local,\n",
    "                'download_enable': '0',\n",
    "                'book_on_rent': '0',\n",
    "                'book_rent_price': None,  # MySQL NULL equivalent in Python\n",
    "                'book_rent_time': None,  # MySQL NULL equivalent in Python\n",
    "                'featured': '0',  \n",
    "                'status': '1'\n",
    "            }\n",
    "\n",
    "def move_and_rename_epub(source_path, destination_directory, new_name):\n",
    "    \"\"\"\n",
    "    Move an EPUB file from the source path to the destination directory and rename it.\n",
    "\n",
    "    Args:\n",
    "        source_path (str): The path to the source EPUB file.\n",
    "        destination_directory (str): The path to the destination directory.\n",
    "        new_name (str): The new name for the EPUB file (including .epub extension).\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the newly moved and renamed EPUB file.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(source_path):\n",
    "        print(f\"Source file '{source_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.isdir(destination_directory):\n",
    "        print(f\"Destination directory '{destination_directory}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    new_file_path = os.path.join(destination_directory, new_name)\n",
    "\n",
    "    try:\n",
    "        shutil.move(source_path, new_file_path)\n",
    "        print(f\"File moved and renamed to '{new_file_path}'.\")\n",
    "        return new_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving or renaming file: {e}\")\n",
    "        return None\n",
    "\n",
    "def search_book_id(book_id,csv_file = 'tbl_books.csv'):\n",
    "    \"\"\"\n",
    "    Search for a specific book ID in the 'tbl_books.csv' file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing book data.\n",
    "        book_id (int or str): The book ID to search for in the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the book ID is found, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file cannot be found or opened.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            \n",
    "            # Loop through each row in the CSV\n",
    "            for row in reader:\n",
    "                # Check if the current row's 'id' matches the given book_id\n",
    "                if row['id'] == str(book_id):\n",
    "                    return True  # Return True if the book_id is found\n",
    "        return False  # Return False if not found\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_file} was not found.\")\n",
    "        return False            \n",
    "\n",
    "def check_book_id(book_id,csv_file):\n",
    "    \"\"\"\n",
    "    Search for a specific book ID in the 'tbl_books.csv' file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing book data.\n",
    "        book_id (int or str): The book ID to search for in the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the book ID is found, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file cannot be found or opened.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            \n",
    "            # Loop through each row in the CSV\n",
    "            for row in reader:\n",
    "                # Check if the current row's 'id' matches the given book_id\n",
    "                if row['id'] == str(book_id):\n",
    "                    return True  # Return True if the book_id is found\n",
    "        return False  # Return False if not found\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_file} was not found.\")\n",
    "        return False            \n",
    "\n",
    "def check_and_append_aid(aid, csv_file='aid1.csv'):\n",
    "    \"\"\"\n",
    "    Checks if an author ID exists in the CSV file, and if not, appends it on a new line.\n",
    "    If the file is empty, adds a header before appending the ID.\n",
    "\n",
    "    Args:\n",
    "        aid (str): The author ID to search for.\n",
    "        csv_file (str): The path to the CSV file (default is 'aid1.csv').\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the ID was appended, False if it already existed.\n",
    "    \"\"\"\n",
    "    aid_exists = False\n",
    "    \n",
    "    # Check if the file exists and is empty\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    file_empty = os.path.getsize(csv_file) == 0 if file_exists else True\n",
    "\n",
    "    # Read the CSV and check if the ID exists\n",
    "    if not file_empty:\n",
    "        try:\n",
    "            with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    if row['aid'] == str(aid):\n",
    "                        aid_exists = True\n",
    "                        break\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {csv_file} not found, creating a new one.\")\n",
    "\n",
    "    # If the ID doesn't exist, append it to the file\n",
    "    if not aid_exists:\n",
    "        with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if file_empty:  # Add header if the file is empty or newly created\n",
    "                writer.writerow(['aid'])\n",
    "            writer.writerow([aid])  # Append the ID on a new line\n",
    "        print(f\"Appended ID {aid} to {csv_file}.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"ID {aid} already exists in {csv_file}.\")\n",
    "        return False\n",
    "\n",
    "def append_to_book_csv(book_data, csv_file):\n",
    "    \"\"\"\n",
    "    Appends the book data to the specified CSV file.\n",
    "\n",
    "    Args:\n",
    "        book_data (dict): A dictionary containing book details.\n",
    "        csv_file (str): The path to the CSV file where the data should be appended.\n",
    "    \"\"\"\n",
    "    fieldnames = [\"id\", \"cat_id\", \"sub_cat_id\", \"author_ids\", \"book_access\", \"title\", \n",
    "              \"description\", \"image\", \"url_type\", \"url\", \"download_enable\", \n",
    "              \"book_on_rent\", \"book_rent_price\", \"book_rent_time\", \"featured\", \"status\"]\n",
    "\n",
    "    # Check if the file exists to write the header only if it's a new file\n",
    "    try:\n",
    "        with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "            # Write header only if file is empty or does not exist\n",
    "            if file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "\n",
    "            # Write the book data\n",
    "            writer.writerow(book_data)\n",
    "            print(f\"Appended data for book '{book_data['title']}' to {csv_file}.\")\n",
    "    \n",
    "    except IOError as e:\n",
    "        print(f\"An I/O error occurred: {e}\")\n",
    "        \n",
    "def clean_url(url):\n",
    "    \"\"\"\n",
    "    Cleans the URL by removing everything after the '?'.\n",
    "\n",
    "    Args:\n",
    "        url (str): The input URL to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned URL with everything after '?' removed.\n",
    "    \"\"\"\n",
    "    # Parse the URL\n",
    "    parsed_url = urlparse(url)\n",
    "    # Construct the cleaned URL by using the scheme, netloc, path, and excluding the query and fragment\n",
    "    cleaned_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}\"\n",
    "    return cleaned_url\n",
    "\n",
    "def extract_search_query(url):\n",
    "    \"\"\"\n",
    "    Extracts the search query from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL containing the search query.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted search query, or None if no query is found.\n",
    "    \"\"\"\n",
    "    # Parse the URL and extract the query parameters\n",
    "    parsed_url = urlparse(url)\n",
    "    \n",
    "    # Parse the query string into a dictionary\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    \n",
    "    # Get the 'q' parameter, which is typically used for search queries\n",
    "    search_query = query_params.get('q')\n",
    "    \n",
    "    # Return the search query if it exists, otherwise return None\n",
    "    return search_query[0] if search_query else None\n",
    "\n",
    "def remove_extra_spaces(s):\n",
    "    \"\"\"\n",
    "    Removes extra spaces from the string and ensures only single spaces between words.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string with extra spaces.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned-up string with extra spaces removed.\n",
    "    \"\"\"\n",
    "    # Split the string into words, automatically removing extra spaces\n",
    "    words = s.split()\n",
    "    # Join the words with a single space\n",
    "    cleaned_string = ' '.join(words)\n",
    "    return cleaned_string\n",
    "\n",
    "def scrape_goodreads_books(url, author_name):\n",
    "    \"\"\"\n",
    "    Scrapes Goodreads books from the provided URL and compares each book's author(s) with the given author name.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the Goodreads search results page.\n",
    "        author_name (str): The author name to compare with the book authors.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of URLs of the books found on the page.\n",
    "    \"\"\"\n",
    "    source = urlopen(url)  # Open the URL and retrieve the HTML source\n",
    "    soup = BeautifulSoup(source, \"html.parser\")  # Parse the HTML with BeautifulSoup\n",
    "    # Find all book containers on the page\n",
    "    book_containers = soup.find_all('tr', itemtype='http://schema.org/Book')\n",
    "    \n",
    "    print(f'Results found: {len(book_containers)}')\n",
    "    #log(f'Results found: {len(book_containers)}')\n",
    "    \n",
    "    # If only one result is found, return its URL immediately\n",
    "    if len(book_containers) == 1:\n",
    "        container = book_containers[0]\n",
    "        title_link = container.find('a', class_='bookTitle').get(\"href\")\n",
    "        complete_book_url = clean_url(GOODREADS_URL + title_link)\n",
    "        return [complete_book_url]\n",
    "    \n",
    "    # List to store all book URLs\n",
    "    complete_book_urls = []\n",
    "    search_query = extract_search_query(url)\n",
    "    \n",
    "    print(search_query)\n",
    "    \n",
    "    # Iterate over each book container\n",
    "    for container in book_containers:\n",
    "        # Extract book title link and title\n",
    "        title_link = container.find('a', class_='bookTitle').get(\"href\")\n",
    "        title = container.find('a', class_='bookTitle').text.strip()\n",
    "        \n",
    "        # Extract book author(s)\n",
    "        author_containers = container.find_all('a', class_='authorName')\n",
    "        authors = [author.text.strip() for author in author_containers]\n",
    "        \n",
    "        #print(authors)\n",
    "        \n",
    "        # Check if the given author_name matches any of the extracted authors\n",
    "        if any(author_name.lower().strip() in remove_extra_spaces(author.lower().strip()) for author in authors):\n",
    "            # Extract the book rating\n",
    "            rating = container.find('span', class_='minirating').text.strip()\n",
    "                \n",
    "            # Extract the image URL\n",
    "            image_url_tag = container.find('img', itemprop='image')\n",
    "            image_url = image_url_tag['src'] if image_url_tag else 'No image available'\n",
    "                \n",
    "            # Construct the full book URL\n",
    "            complete_book_url = clean_url(GOODREADS_URL + title_link)\n",
    "            \n",
    "            # Add the complete book URL to the list\n",
    "            complete_book_urls.append(complete_book_url)\n",
    "            return complete_book_url\n",
    "        \n",
    "        complete_book_url = clean_url(GOODREADS_URL + title_link)\n",
    "        # Add the complete book URL to the list\n",
    "        complete_book_urls.append(complete_book_url)\n",
    "    if complete_book_urls:\n",
    "    # Return the list of all book URLs\n",
    "        return complete_book_urls[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_epub_info(fname):\n",
    "    \"\"\"\n",
    "    Extracts metadata from an EPUB file.\n",
    "\n",
    "    Args:\n",
    "        fname (str): The path to the EPUB file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted metadata, including 'title' and 'creator',\n",
    "              or None if an error occurs.\n",
    "    \"\"\"\n",
    "    ns = {\n",
    "        # Namespace for the container.xml file\n",
    "        'n': 'urn:oasis:names:tc:opendocument:xmlns:container',\n",
    "        'pkg': 'http://www.idpf.org/2007/opf',  # Namespace for the package metadata\n",
    "        'dc': 'http://purl.org/dc/elements/1.1/'  # Namespace for Dublin Core metadata\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Open the EPUB file\n",
    "        zip = zipfile.ZipFile(fname)\n",
    "\n",
    "        # Read the content of the container.xml file\n",
    "        txt = zip.read('META-INF/container.xml')\n",
    "        tree = etree.fromstring(txt)  # Parse the XML content\n",
    "\n",
    "        # Extract the path of the contents metafile\n",
    "        cfname = tree.xpath('n:rootfiles/n:rootfile/@full-path', namespaces=ns)[0]\n",
    "\n",
    "        # Read the contents metafile\n",
    "        cf = zip.read(cfname)  # Read the contents metafile\n",
    "        tree = etree.fromstring(cf)  # Parse the XML content\n",
    "\n",
    "        # Extract the metadata block\n",
    "        p = tree.xpath('/pkg:package/pkg:metadata', namespaces=ns)[0]\n",
    "\n",
    "        # Repackage the data\n",
    "        res = {}  # Initialize a dictionary to store the metadata\n",
    "        for s in ['title', 'creator']:\n",
    "            # Extract the text content of each metadata element\n",
    "            res[s] = p.xpath(f'dc:{s}/text()', namespaces=ns)[0]\n",
    "\n",
    "        return res  # Return the metadata dictionary\n",
    "\n",
    "    except (zipfile.BadZipFile, KeyError, etree.XMLSyntaxError, IndexError) as e:\n",
    "        # Handle specific exceptions (e.g., invalid EPUB format, missing metadata)\n",
    "        print(f\"Error reading EPUB file '{fname}': {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Handle other unexpected exceptions\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "def process_book_search(book_name, book_url_local, author_name, current_csv, list_of_user_agents, file):\n",
    "    \"\"\"\n",
    "    Process a book search to find and scrape book information from Goodreads.\n",
    "\n",
    "    Parameters:\n",
    "    - book_name (str): The name of the book to search for.\n",
    "    - book_url_local (str): The local URL or filename of the book.\n",
    "    - author_name (str): The author of the book.\n",
    "    - current_csv (str): The name of the CSV file to append book information.\n",
    "    - list_of_user_agents (list): A list of user agent strings for HTTP requests.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a folder for the author inside the 'uploads' directory if the author name is not 'Unknown'\n",
    "    if author_name.lower() != 'unknown':\n",
    "        dest = os.path.join('upload', author_name)\n",
    "        if not os.path.exists(dest):\n",
    "            os.makedirs(dest)\n",
    "\n",
    "    print('Trying Goodreads search again')\n",
    "    base_url = 'https://www.goodreads.com/search?'\n",
    "    params = {'q': book_name}\n",
    "    search_url = base_url + urllib.parse.urlencode(params)\n",
    "    print(search_url)\n",
    "\n",
    "    ebook_url = scrape_goodreads_books(search_url, author_name)\n",
    "    if ebook_url:\n",
    "            if isinstance(ebook_url, list):\n",
    "                ebook_url = ebook_url[0]\n",
    "            print('Link for ' + str(book_url_local) + ' found at ' + str(ebook_url))\n",
    "            user_agent = random.choice(list_of_user_agents)\n",
    "            random_header = {'User-Agent': user_agent}\n",
    "            new_author_id=''\n",
    "\n",
    "            try:\n",
    "                book_info = scrape_book(ebook_url, book_url_local, random_header)\n",
    "                if not search_book_id(book_info['id']) and not check_book_id(book_info['id'], current_csv):\n",
    "                    authorid = book_info[\"author_ids\"]\n",
    "                    # Check if authorid is a comma-separated list\n",
    "                    if ',' in authorid:\n",
    "                        author_ids = authorid.split(',')\n",
    "                        for author in author_ids:\n",
    "                            check_and_append_aid(author.strip(), csv_file='aid1.csv')\n",
    "                        # If author name is unknown, save the first author ID in new_author_id\n",
    "                        if author_name.lower() == 'unknown':\n",
    "                            new_author_id = author_ids[0].strip()\n",
    "                    else:\n",
    "                        # If it's a single author ID, process it normally\n",
    "                        check_and_append_aid(authorid.strip(), csv_file='aid1.csv')\n",
    "                        # If author name is unknown, save the author ID in new_author_id\n",
    "                        if author_name.lower() == 'unknown':\n",
    "                            new_author_id = authorid.strip()\n",
    "                    if author_name.lower() == 'unknown':\n",
    "                        author_data = scrape_author(new_author_id)\n",
    "                        if author_data:\n",
    "                            author_name1 = author_data['name']\n",
    "                            dest = os.path.join('upload', author_name1)\n",
    "                            if not os.path.exists(dest):\n",
    "                                os.makedirs(dest)\n",
    "                            \n",
    "                    print('Book scraped ----> ' + book_info['id'])\n",
    "                    new_file_path=move_and_rename_epub(file, dest, book_url_local)\n",
    "                    book_info['url'] = new_file_path\n",
    "                    #print('Book file moved to ' + new_file_path)\n",
    "                    append_to_book_csv(book_info, current_csv)\n",
    "                    print('=========================> Scraping ' + book_name + ' done')\n",
    "                else:\n",
    "                    print(f\"{book_info['title']} already added.....skipping.\")\n",
    "                    os.remove(file)\n",
    "                    print('book deleted')\n",
    "            except ConnectionError as e:\n",
    "                if 'HTTPSConnectionPool' in str(e):\n",
    "                    # Check if failedbooks.csv exists, if not create and write the header\n",
    "                    failed_books_file = 'failedbooks.csv'\n",
    "                    if not os.path.exists(failed_books_file):\n",
    "                        with open(failed_books_file, mode='w', newline='', encoding='utf-8') as failed_file:\n",
    "                            writer = csv.writer(failed_file)\n",
    "                            writer.writerow(['book_name', 'book_url_local', 'author_name', 'file', 'result_book_url'])  # Write header\n",
    "\n",
    "                    # Save failed book details to failedbooks.csv\n",
    "                    with open(failed_books_file, mode='a', newline='', encoding='utf-8') as failed_file:\n",
    "                        writer = csv.writer(failed_file)\n",
    "                        writer.writerow([book_name, book_url_local, author_name, file, ebook_url])\n",
    "                    print(f\"ConnectionError: HTTPSConnectionPool occurred for {book_name}. Details saved to failedbooks.csv. Skipping this book.\")\n",
    "                    return  # Continue with the next book in the loop\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while scraping the book {book_name}: {e}\")\n",
    "                failed_books_file = 'failedbooks.csv'\n",
    "                if not os.path.exists(failed_books_file):\n",
    "                    with open(failed_books_file, mode='w', newline='', encoding='utf-8') as failed_file:\n",
    "                        writer = csv.writer(failed_file)\n",
    "                        writer.writerow(['book_name', 'book_url_local', 'author_name', 'file', 'result_book_url'])  # Write header\n",
    "\n",
    "                # Save failed book details to failedbooks.csv\n",
    "                with open(failed_books_file, mode='a', newline='', encoding='utf-8') as failed_file:\n",
    "                    writer = csv.writer(failed_file)\n",
    "                    writer.writerow([book_name, book_url_local, author_name, file, ebook_url])\n",
    "                return  # Continue with the next book in the loop\n",
    "    else:\n",
    "            print(f\"======={book_url_local} Tried againg======>.\")\n",
    "            new_url=book_google_search(book_name)\n",
    "            if new_url:\n",
    "                print('Link for ' + str(book_url_local) + ' found at ' + str(new_url))\n",
    "                user_agent = random.choice(list_of_user_agents)\n",
    "                random_header = {'User-Agent': user_agent}\n",
    "                try:\n",
    "                    book_info = scrape_book(new_url, book_url_local, random_header)\n",
    "                    if not search_book_id(book_info['id']) and not check_book_id(book_info['id'], current_csv):\n",
    "                        authorid = book_info[\"author_ids\"]\n",
    "                        # Check if authorid is a comma-separated list\n",
    "                        if ',' in authorid:\n",
    "                            author_ids = authorid.split(',')\n",
    "                            for author in author_ids:\n",
    "                                check_and_append_aid(author.strip(), csv_file='aid1.csv')\n",
    "                            # If author name is unknown, save the first author ID in new_author_id\n",
    "                            if author_name.lower() == 'unknown':\n",
    "                                new_author_id = author_ids[0].strip()\n",
    "                        else:\n",
    "                            # If it's a single author ID, process it normally\n",
    "                            check_and_append_aid(authorid.strip(), csv_file='aid1.csv')\n",
    "                            # If author name is unknown, save the author ID in new_author_id\n",
    "                            if author_name.lower() == 'unknown':\n",
    "                                new_author_id = authorid.strip()\n",
    "                        if author_name.lower() == 'unknown':\n",
    "                            author_data = scrape_author(new_author_id)\n",
    "                            if author_data:\n",
    "                                author_name1 = author_data['name']\n",
    "                                print(author_name1+\" updated---------------------->\")\n",
    "                                dest = os.path.join('upload', author_name1)\n",
    "                                if not os.path.exists(dest):\n",
    "                                    os.makedirs(dest)\n",
    "                        print('Book scraped ----> ' + book_info['id'])\n",
    "                        new_file_path=move_and_rename_epub(file, dest, book_url_local)\n",
    "                        book_info['url'] = new_file_path\n",
    "                        #print('Book file moved to ' + new_file_path)\n",
    "                        append_to_book_csv(book_info, current_csv)\n",
    "                        print('=========================> Scraping ' + book_name + ' done')\n",
    "                    else:\n",
    "                        print(f\"{book_info['title']} already added.....skipping.\")\n",
    "                        os.remove(file)\n",
    "                        print('book deleted')\n",
    "                except ConnectionError as e:\n",
    "                    if 'HTTPSConnectionPool' in str(e):\n",
    "                        # Check if failedbooks.csv exists, if not create and write the header\n",
    "                        failed_books_file = 'failedbooks.csv'\n",
    "                        if not os.path.exists(failed_books_file):\n",
    "                            with open(failed_books_file, mode='w', newline='', encoding='utf-8') as failed_file:\n",
    "                                writer = csv.writer(failed_file)\n",
    "                                writer.writerow(['book_name', 'book_url_local', 'author_name', 'file', 'result_book_url'])\n",
    "            else:\n",
    "                print(f\"Book not found for {book_name} in Google search. Skipping this book.\")\n",
    "      \n",
    "\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125c858",
   "metadata": {},
   "source": [
    "### split_epub_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdaeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting path\n",
    "starts = r'sample2'\n",
    "current_folder = os.path.basename(starts)\n",
    "\n",
    "# Create the 'books' directory if it doesn't exist\n",
    "books_folder = os.path.join(os.getcwd(), 'books')\n",
    "os.makedirs(books_folder, exist_ok=True)\n",
    "\n",
    "# Set the path for the current CSV file in the 'books' directory\n",
    "current_csv = os.path.join(books_folder, current_folder.replace(' ', '_') + '.csv')\n",
    "\n",
    "# Create the CSV file if it doesn't exist\n",
    "if not os.path.exists(current_csv):\n",
    "    with open(current_csv, mode='w', newline='') as csv_file:\n",
    "        pass\n",
    "\n",
    "# Initialize the counter for processed books\n",
    "processed_count = 0\n",
    "\n",
    "# Loop through all files in the starting directory\n",
    "for root, dirs, files in os.walk(starts):\n",
    "    for file in files:\n",
    "        if file.endswith('.epub') or file.endswith('.ePub'):\n",
    "            new_file_name11 = file.replace('-OceanofPDF.com-', '')\n",
    "            new_file_name22 = new_file_name11.replace('_OceanofPDF.com_', '')\n",
    "            new_file_name = new_file_name22.replace(' ', '_')\n",
    "            book_url_local = new_file_name\n",
    "            new_creator_name = ''\n",
    "            \n",
    "            if not is_book_in_csv(book_url_local, current_csv):\n",
    "                epub_path = os.path.join(root, file)\n",
    "                author_title = split_epub_filename(clean_ebook_filename(new_file_name22))\n",
    "                \n",
    "                if author_title:\n",
    "                    ebook_title = author_title['title']\n",
    "                    ebook_creator = author_title['author']\n",
    "                    new_creator_name = ebook_creator\n",
    "                    ebook_name_clean = re.sub(r'\\(.*?\\)', '', ebook_title)\n",
    "                    bookname = ebook_name_clean + ' by ' + new_creator_name\n",
    "                    print(f'----> working on {bookname}')\n",
    "\n",
    "                    \n",
    "                    # Process the book search\n",
    "                    #process_book_search(bookname, book_url_local, new_creator_name, current_csv, list_of_user_agents, epub_path)\n",
    "                    \n",
    "                    # Increment the processed books count\n",
    "                    processed_count += 1\n",
    "                    print(f\"Processed {processed_count} books so far.\")\n",
    "                else:\n",
    "                    print(f\"{book_url_local} info corrupted.. skipping.\")\n",
    "                    pass\n",
    "            else:\n",
    "                print(f\"{book_url_local} is already in the CSV file.\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d8506",
   "metadata": {},
   "source": [
    "### get_epub_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df799748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting path\n",
    "starts = r'All_Books'\n",
    "current_folder = os.path.basename(starts)\n",
    "\n",
    "# Create the 'books' directory if it doesn't exist\n",
    "books_folder = os.path.join(os.getcwd(), 'books')\n",
    "os.makedirs(books_folder, exist_ok=True)\n",
    "\n",
    "# Set the path for the current CSV file in the 'books' directory\n",
    "current_csv = os.path.join(books_folder, current_folder.replace(' ', '_') + '.csv')\n",
    "\n",
    "# Create the CSV file if it doesn't exist\n",
    "if not os.path.exists(current_csv):\n",
    "    with open(current_csv, mode='w', newline='') as csv_file:\n",
    "        pass\n",
    "\n",
    "# Initialize the counter for processed books\n",
    "processed_books_count = 0\n",
    "\n",
    "for root, dirs, files in os.walk(starts):\n",
    "    # Loop through all files in the current directory\n",
    "    for file in files:\n",
    "        # Check if the file is an EPUB file\n",
    "        if file.endswith('.epub') or file.endswith('.ePub'):\n",
    "            new_file_name11 = file.replace('-OceanofPDF.com-', '')\n",
    "            new_file_name22 = new_file_name11.replace('_OceanofPDF.com_', '')\n",
    "            new_file_name = new_file_name22.replace(' ', '_')\n",
    "            book_url_local = new_file_name\n",
    "            new_creator_name = ''\n",
    "\n",
    "            if not is_book_in_csv(book_url_local, current_csv):\n",
    "                # Create the full path of the EPUB file\n",
    "                epub_path = os.path.join(root, file)\n",
    "\n",
    "                ebook_info = get_epub_info(epub_path)\n",
    "                if ebook_info:\n",
    "                    #print(ebook_info)\n",
    "                    ebook_title = ebook_info['title']\n",
    "                    #print('----> working on ' + ebook_title)\n",
    "                    ebook_name_clean = re.sub(r'\\(.*?\\)', '', ebook_title)\n",
    "                    if ':' in ebook_name_clean:\n",
    "                        ebook_name_clean = ebook_name_clean.split(':')[0]\n",
    "                    ebook_creator = ebook_info['creator']\n",
    "                    new_creator_name = ebook_creator\n",
    "                    if ',' in ebook_creator:\n",
    "                        creator_names = ebook_creator.split(',')\n",
    "                        first_name = creator_names[1].strip()\n",
    "                        second_name = creator_names[0].strip()\n",
    "                        print(first_name, second_name)\n",
    "                        new_creator_name = first_name + ' ' + second_name\n",
    "                    bookname = ebook_name_clean + ' by ' + new_creator_name\n",
    "                    #print(new_creator_name)  \n",
    "                    # Step 1: Remove leading numbers, dots, dashes, and whitespace\n",
    "                    clean_title = re.sub(r'^[\\d.\\-\\s]+', '', ebook_name_clean)\n",
    "\n",
    "                    # Step 2: Remove any parenthetical text (e.g., \"(Heart Series, #1)\")\n",
    "                    clean_title = re.sub(r'\\s*\\([^)]*\\)', '', clean_title).strip()\n",
    "\n",
    "                    # Final formatted book name\n",
    "                    bookname = clean_title + ' by ' + new_creator_name\n",
    "                    \n",
    "                                \n",
    "                    if new_creator_name.lower() == 'unknown':\n",
    "                        print(f\"{book_url_local} still has an unknown Author.\")\n",
    "                        bookname = ebook_name_clean\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    print(f'----> working on {bookname}')  \n",
    "                    # Create a folder for the author inside the 'uploads' directory\n",
    "                    process_book_search(bookname, book_url_local, new_creator_name, current_csv, list_of_user_agents, epub_path)\n",
    "                    \n",
    "\n",
    "\n",
    "                    # Increment the counter for each successfully processed book\n",
    "                    processed_books_count += 1\n",
    "\n",
    "                    # Show the count after each book is processed\n",
    "                    print(f\"Processed books count: {processed_books_count}\")\n",
    "                else:\n",
    "                    print(f\"{book_url_local} info corrupted.. skipping.\")\n",
    "                    pass\n",
    "            else:\n",
    "                print(f\"{book_url_local} is already in the CSV file.\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2b8ad",
   "metadata": {},
   "source": [
    "### scrape_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5722bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file1 = 'authorsoceanofpdf.csv'\n",
    "csv_file = csv.reader(open('aid1.csv', \"r\", encoding='utf-8'), delimiter=\",\")\n",
    "next(csv_file)\n",
    "for row in csv_file:\n",
    "    if row:\n",
    "        a_id = row[0].strip()\n",
    "        if not is_author_id_in_csv(a_id) and not is_author_id_in_local_csv(a_id, csv_file1):\n",
    "            print('========> adding author id '+a_id)\n",
    "            author_data = scrape_author(a_id)  # Scrape author data\n",
    "            append_author_to_csv('authorsoceanofpdf.csv', author_data)\n",
    "            print(f\"+++++++++> {author_data['name']} added\")\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            print(f\"{a_id} is already in the CSV file.\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ffa72",
   "metadata": {},
   "source": [
    "### sort by filesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff1c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting path\n",
    "starts = r'C:\\xampp8.2\\htdocs\\php_web_services\\uploads'\n",
    "current_folder = os.path.basename(starts)\n",
    "\n",
    "# Create the 'books' directory if it doesn't exist\n",
    "books_folder = os.path.join(os.getcwd(), 'books')\n",
    "os.makedirs(books_folder, exist_ok=True)\n",
    "\n",
    "# Set the path for the current CSV file in the 'books' directory\n",
    "current_csv = os.path.join(books_folder, current_folder.replace(' ', '_') + '.csv')\n",
    "\n",
    "# Create the CSV file if it doesn't exist\n",
    "if not os.path.exists(current_csv):\n",
    "    with open(current_csv, mode='w', newline='') as csv_file:\n",
    "        pass\n",
    "\n",
    "# Initialize the counter for processed books\n",
    "processed_books_count = 0\n",
    "search_csv = r'tbl_books_old.csv'\n",
    "\n",
    "# Collect all files with their sizes\n",
    "file_info = []\n",
    "for root, dirs, files in os.walk(starts):\n",
    "    for file in files:\n",
    "        if file.endswith('.epub') or file.endswith('.ePub'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_size = os.path.getsize(file_path)  # Get file size in bytes\n",
    "            file_info.append((file, file_path, file_size))\n",
    "\n",
    "# Sort files by size in descending order\n",
    "file_info.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Process files in sorted order\n",
    "for file, file_path, file_size in file_info:\n",
    "    if file.endswith('.epub') or file.endswith('.ePub'):\n",
    "            print(f\"Processing file: {file}\")\n",
    "            book_url_local=file\n",
    "            epub_path = os.path.join(root, file)\n",
    "            matching_rows = find_book_by_file_url_in_csv(search_csv, file)\n",
    "            if matching_rows:\n",
    "                print(f\"============>>> File '{file}' found in {search_csv}.\")\n",
    "                book_data_df=match_book(matching_rows)\n",
    "                add_books_to_uploads(book_data_df,epub_path,current_csv)\n",
    "                # Increment the counter for each successfully processed book\n",
    "                processed_books_count += 1\n",
    "\n",
    "                # Show the count after each book is processed\n",
    "                print(f\"Processed books count: {processed_books_count}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"File '{file}' not found in {search_csv}. Processing further.\")\n",
    "                new_creator_name = ''\n",
    "\n",
    "                if not is_book_in_csv(file, current_csv):\n",
    "                    # Create the full path of the EPUB file\n",
    "                    epub_path = os.path.join(root, file)\n",
    "\n",
    "                    ebook_info = get_epub_info(epub_path)\n",
    "                    if ebook_info:\n",
    "                        #print(ebook_info)\n",
    "                        ebook_title = ebook_info['title']\n",
    "                        #print('----> working on ' + ebook_title)\n",
    "                        ebook_name_clean = re.sub(r'\\(.*?\\)', '', ebook_title)\n",
    "                        if ':' in ebook_name_clean:\n",
    "                            ebook_name_clean = ebook_name_clean.split(':')[0]\n",
    "                        ebook_creator = ebook_info['creator']\n",
    "                        new_creator_name = ebook_creator\n",
    "                        if ',' in ebook_creator:\n",
    "                            creator_names = ebook_creator.split(',')\n",
    "                            first_name = creator_names[1].strip()\n",
    "                            second_name = creator_names[0].strip()\n",
    "                            print(first_name, second_name)\n",
    "                            new_creator_name = first_name + ' ' + second_name\n",
    "                        bookname = ebook_name_clean + ' by ' + new_creator_name\n",
    "                        #print(new_creator_name)  \n",
    "                        # Step 1: Remove leading numbers, dots, dashes, and whitespace\n",
    "                        clean_title = re.sub(r'^[\\d.\\-\\s]+', '', ebook_name_clean)\n",
    "\n",
    "                        # Step 2: Remove any parenthetical text (e.g., \"(Heart Series, #1)\")\n",
    "                        clean_title = re.sub(r'\\s*\\([^)]*\\)', '', clean_title).strip()\n",
    "\n",
    "                        # Final formatted book name\n",
    "                        bookname = clean_title + ' by ' + new_creator_name\n",
    "                        \n",
    "                                    \n",
    "                        if new_creator_name.lower() == 'unknown':\n",
    "                            print(f\"{book_url_local} still has an unknown Author.\")\n",
    "                            bookname = ebook_name_clean\n",
    "                        \n",
    "\n",
    "                        \n",
    "                        print(f'----> working on {bookname}')  \n",
    "                        # Create a folder for the author inside the 'uploads' directory\n",
    "                        process_book_search(bookname, book_url_local, new_creator_name, current_csv, list_of_user_agents, epub_path)\n",
    "                        # Increment the counter for each successfully processed book\n",
    "                        processed_books_count += 1\n",
    "\n",
    "                        # Show the count after each book is processed\n",
    "                        print(f\"Processed books count: {processed_books_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a9903",
   "metadata": {},
   "source": [
    "### Processed old books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting path\n",
    "starts = r'C:\\xampp8.2\\htdocs\\php_web_services\\uploads'\n",
    "current_folder = os.path.basename(starts)\n",
    "\n",
    "# Create the 'books' directory if it doesn't exist\n",
    "books_folder = os.path.join(os.getcwd(), 'books')\n",
    "os.makedirs(books_folder, exist_ok=True)\n",
    "\n",
    "# Set the path for the current CSV file in the 'books' directory\n",
    "current_csv = os.path.join(books_folder, current_folder.replace(' ', '_') + '.csv')\n",
    "\n",
    "# Create the CSV file if it doesn't exist\n",
    "if not os.path.exists(current_csv):\n",
    "    with open(current_csv, mode='w', newline='') as csv_file:\n",
    "        pass\n",
    "\n",
    "# Initialize the counter for processed books\n",
    "processed_books_count = 0\n",
    "search_csv = r'tbl_books_old.csv'\n",
    "\n",
    "for root, dirs, files in os.walk(starts):\n",
    "    # Loop through all files in the current directory\n",
    "    for file in files:\n",
    "        # Check if the file is an EPUB file\n",
    "        if file.endswith('.epub') or file.endswith('.ePub'):\n",
    "            print(f\"Processing file: {file}\")\n",
    "            book_url_local=file\n",
    "            epub_path = os.path.join(root, file)\n",
    "            matching_rows = find_book_by_file_url_in_csv(search_csv, file)\n",
    "            if matching_rows:\n",
    "                print(f\"============>>> File '{file}' found in {search_csv}.\")\n",
    "                book_data_df=match_book(matching_rows)\n",
    "                add_books_to_uploads(book_data_df,epub_path,current_csv)\n",
    "                # Increment the counter for each successfully processed book\n",
    "                processed_books_count += 1\n",
    "\n",
    "                # Show the count after each book is processed\n",
    "                print(f\"Processed books count: {processed_books_count}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"File '{file}' not found in {search_csv}. Processing further.\")\n",
    "                new_creator_name = ''\n",
    "\n",
    "                if not is_book_in_csv(file, current_csv):\n",
    "                    # Create the full path of the EPUB file\n",
    "                    epub_path = os.path.join(root, file)\n",
    "\n",
    "                    ebook_info = get_epub_info(epub_path)\n",
    "                    if ebook_info:\n",
    "                        #print(ebook_info)\n",
    "                        ebook_title = ebook_info['title']\n",
    "                        #print('----> working on ' + ebook_title)\n",
    "                        ebook_name_clean = re.sub(r'\\(.*?\\)', '', ebook_title)\n",
    "                        if ':' in ebook_name_clean:\n",
    "                            ebook_name_clean = ebook_name_clean.split(':')[0]\n",
    "                        ebook_creator = ebook_info['creator']\n",
    "                        new_creator_name = ebook_creator\n",
    "                        if ',' in ebook_creator:\n",
    "                            creator_names = ebook_creator.split(',')\n",
    "                            first_name = creator_names[1].strip()\n",
    "                            second_name = creator_names[0].strip()\n",
    "                            print(first_name, second_name)\n",
    "                            new_creator_name = first_name + ' ' + second_name\n",
    "                        bookname = ebook_name_clean + ' by ' + new_creator_name\n",
    "                        #print(new_creator_name)  \n",
    "                        # Step 1: Remove leading numbers, dots, dashes, and whitespace\n",
    "                        clean_title = re.sub(r'^[\\d.\\-\\s]+', '', ebook_name_clean)\n",
    "\n",
    "                        # Step 2: Remove any parenthetical text (e.g., \"(Heart Series, #1)\")\n",
    "                        clean_title = re.sub(r'\\s*\\([^)]*\\)', '', clean_title).strip()\n",
    "\n",
    "                        # Final formatted book name\n",
    "                        bookname = clean_title + ' by ' + new_creator_name\n",
    "                        \n",
    "                                    \n",
    "                        if new_creator_name.lower() == 'unknown':\n",
    "                            print(f\"{book_url_local} still has an unknown Author.\")\n",
    "                            bookname = ebook_name_clean\n",
    "                        \n",
    "\n",
    "                        \n",
    "                        print(f'----> working on {bookname}')  \n",
    "                        # Create a folder for the author inside the 'uploads' directory\n",
    "                        process_book_search(bookname, book_url_local, new_creator_name, current_csv, list_of_user_agents, epub_path)\n",
    "                        # Increment the counter for each successfully processed book\n",
    "                        processed_books_count += 1\n",
    "\n",
    "                        # Show the count after each book is processed\n",
    "                        print(f\"Processed books count: {processed_books_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e73980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in books_final2.csv: 1983\n"
     ]
    }
   ],
   "source": [
    "#clean_url_column1('books/uploads.csv', 'books/uploads_redone1.csv')\n",
    "row_count = count_rows_in_csv('books/uploads.csv')\n",
    "print(f\"Total rows in books_final2.csv: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab79f99",
   "metadata": {},
   "source": [
    "### edit sub categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/content/drive/MyDrive/genre_codes/books_final2.csv\n",
    "csv_file = csv.reader(open('books_final2.csv', \"r\", encoding='utf-8'), delimiter=\",\")\n",
    "#id,cat_id,sub_cat_id,author_ids,book_access,title,description,image,url_type,url,download_enable,book_on_rent,book_rent_price,book_rent_time,featured,status\n",
    "current_csv = 'books_final3.csv'\n",
    "counter = 0  # Initialize a counter\n",
    "next(csv_file)\n",
    "for i, row in enumerate(csv_file):\n",
    "    if i >= 2:  # Stop after the first 5 rows\n",
    "        break\n",
    "    if row:\n",
    "        counter += 1  # Increment the counter\n",
    "        author_book_id = row[0].strip()\n",
    "        book_url_local = row[9].strip()\n",
    "        # Check if author_book_id is already in current_csv\n",
    "        if is_book_in_csv(book_url_local, current_csv):\n",
    "            print(f\"Book with ID {author_book_id} is already in the CSV file. Skipping.\")\n",
    "            continue\n",
    "        ebook_url = 'https://www.goodreads.com/book/show/' + author_book_id\n",
    "        print(f'Processing book {counter}: Link found at {ebook_url}')\n",
    "        user_agent = random.choice(list_of_user_agents)\n",
    "        random_header = {'User-Agent': user_agent}\n",
    "        bookpp = scrape_book(ebook_url, book_url_local, random_header)\n",
    "        if bookpp:\n",
    "            print(f'Book scraped ----> {bookpp[\"title\"]}')\n",
    "            bookpp['url'] = book_url_local\n",
    "            append_to_book_csv(bookpp, current_csv)\n",
    "            print(f'=========================> Scraping {book_url_local} done')\n",
    "print(f'Total books processed: {counter}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
